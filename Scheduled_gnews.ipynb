{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aw3t3CeVN9NW"
      },
      "outputs": [],
      "source": [
        "!pip install GoogleNews\n",
        "!pip install gnewsclient\n",
        "!pip install snscrape==0.6.2.20230320\n",
        "!pip install rake_nltk\n",
        "\n",
        "import pandas as pd\n",
        "# from newsapi import NewsApiClient\n",
        "import requests\n",
        "from GoogleNews import GoogleNews\n",
        "from gnewsclient import gnewsclient\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "from datetime import datetime, date\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "import re\n",
        "import numpy as np\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from rake_nltk import Rake\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "id425SVcN9Nb"
      },
      "outputs": [],
      "source": [
        "start_date = []\n",
        "end_date = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9z79OMANN9Nc"
      },
      "outputs": [],
      "source": [
        "sources = [\"bbc-news\", \"the-telegraph\", \"the-guardian-uk\", \"cnn\", \"abc-news-au\",\n",
        "           \"dailymail.co.uk\", \"metro.co.uk\", \"mirror.co.uk\", \"news.google.com\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgr77j2AN9Nd"
      },
      "outputs": [],
      "source": [
        "all_keywords = ['strike', 'holiday', 'lockdown',\n",
        "            'inflation', 'grocery sales', 'carnival', 'festival', 'party', 'Walmart', \"Tesco\", \"Sainsbury's\", \"supply chain\", \"flood\", \"wendys\", \"lidl\"]\n",
        "\n",
        "# all_keywords = ['tesco', 'holiday']\n",
        "\n",
        "# all_keywords = ['autumn', 'bank']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pqjh5tBuy7hC"
      },
      "outputs": [],
      "source": [
        "keywords = ['Lidl','Waitrose','Tesco','Walmart','Sainsbury\\'s', 'Aldi', 'Asda', 'Marks & Spencers', 'Morrison\\'s']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98VGmhUlrzSq"
      },
      "outputs": [],
      "source": [
        "events = ['autumn bank holiday']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlDJce6V8jka"
      },
      "outputs": [],
      "source": [
        "all_events = ['autumn bank holiday']\n",
        "\n",
        "final_prod_events = pd.DataFrame()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKZSWp8ZWckH"
      },
      "outputs": [],
      "source": [
        "counter = 6000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Etrp7MlCN9Ne"
      },
      "outputs": [],
      "source": [
        "gnews_client_topics = ['Top Stories',\n",
        "                       'World',\n",
        "                       'Nation',\n",
        "                       'Business',\n",
        "                       'Technology',\n",
        "                       'Entertainment',\n",
        "                       'Sports',\n",
        "                       'Science',\n",
        "                       'Health']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiYeWUcQUhqR"
      },
      "outputs": [],
      "source": [
        "# branch_keyword_bu_num = {'Esher' : 1, 'Dorchester' : 2}\n",
        "branch_keyword_bu_num = {\n",
        "'Peterborough': 103,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAOuI7dYN9Nf"
      },
      "outputs": [],
      "source": [
        "# England = ['Avon', 'Bedfordshire', 'Berkshire', 'Buckinghamshire', 'Cambridgeshire', 'Cheshire', 'Cleveland',\n",
        "#            'Cornwall', 'Cumbria', 'Derbyshire', 'Devon', 'Dorset', 'Durham', 'East-Sussex', 'Essex', 'Gloucestershire',\n",
        "#            'Hampshire', 'Herefordshire', 'Hertfordshire', 'Isle-of-Wight', 'Kent', 'Lancashire', 'Leice stershire',\n",
        "#            'Lincolnshire', 'London', 'Merseyside',\n",
        "#            'Middlesex', 'Norfolk', 'Northamptonshire', 'Northumberland', 'North-Humberside', 'North-Yorkshire',\n",
        "#            'Nottinghamshire', 'Oxfordshire', 'Rutland', 'Shropshire', 'Somerset', 'South-Humberside', 'South-Yorkshire',\n",
        "#            'Staffordshire', 'Suffolk', 'Surrey', 'Tyne-and-Wear', 'Warwickshire', 'West-Midlands', 'West-Sussex',\n",
        "#            'West-Yorkshire', 'Wiltshire', 'Worcestershire']\n",
        "England = ['London']\n",
        "Wales = ['Clwyd', 'Dyfed', 'Gwent', 'Gwynedd', 'Mid-Glamorgan',\n",
        "         'Powys', 'South-Glamorgan', 'West-Glamorgan']\n",
        "# Wales = ['South-Glamorgan']\n",
        "Scotland = ['Aberdeenshire', 'Angus', 'Argyll', 'Ayrshire', 'Banffshire', 'Berwickshire', 'Bute', 'Caithness',\n",
        "            'Clackmannanshire', 'Dumfriesshire', 'Dunbartonshire', 'East-Lothian', 'Fife', 'Inverness-shire',\n",
        "            'Kincardineshire', 'Kinross-shire',\n",
        "            'Kirkcudbrightshire', 'Lanarkshire', 'Midlothian', 'Moray', 'Nairnshire', 'Orkney', 'Peeblesshire',\n",
        "            'Perthshire', 'Renfrewshire', 'Ross-shire', 'Roxburghshire', 'Selkirkshire', 'Shetland', 'Stirlingshire',\n",
        "            'Sutherland', 'West Lothian', 'Wigtownshire']\n",
        "NorthernIreland = ['Antrim', 'Armagh', 'Down',\n",
        "                   'Fermanagh', 'Londonderry', 'Tyrone']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqosM5ySN9Ng"
      },
      "outputs": [],
      "source": [
        "# branch_keyword = ['Abergavenny', 'Alderley Edge', \"Eastbourne\", \"Edenbridge\", \"Pontprennau\"]\n",
        "# branch_keyword = ['Abingdon', 'Canary Wharf']\n",
        "# all_branch_keyword = ['Yateley', 'Canary Wharf', 'Workingham', 'Firmley']\n",
        "all_branch_keyword = list(branch_keyword_bu_num.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_bhawhRNxnx"
      },
      "outputs": [],
      "source": [
        "branch_keyword = all_branch_keyword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcX9WfgAN9Ng"
      },
      "outputs": [],
      "source": [
        "# countries = [England, Wales, Scotland, NorthernIreland]\n",
        "countries = [England]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPnMjBxmN9Ng"
      },
      "outputs": [],
      "source": [
        "final = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjbT2oHwGenn"
      },
      "outputs": [],
      "source": [
        "# final_prod = pd.DataFrame()\n",
        "status_val = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlTvy7QmN9Nh"
      },
      "outputs": [],
      "source": [
        "def googleNewsByStreet():\n",
        "    data = pd.DataFrame()\n",
        "    for branch in branch_keyword:\n",
        "        for keyword in keywords:\n",
        "            news = GoogleNews()\n",
        "            news.set_date('30/11/2025')\n",
        "            news.get_news(branch + ' ' + keyword)\n",
        "            results = news.result()\n",
        "            df = pd.DataFrame.from_dict(results)\n",
        "            df['keyword'] = keyword\n",
        "            df['branch'] = branch\n",
        "            df['bu_num'] = branch_keyword_bu_num[branch]\n",
        "            print(df)\n",
        "            df.head(5)\n",
        "            data = pd.concat([data, df], ignore_index=True)\n",
        "\n",
        "    # print the dataframe\n",
        "    if len(data.columns) > 4:\n",
        "      data = data.drop(columns=[\"img\", \"site\"])\n",
        "      final.append(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kitk6GatN9Nj"
      },
      "outputs": [],
      "source": [
        "def _removeNonAscii(s):\n",
        "    return \"\".join(i for i in s if ord(i) < 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQ38jsUDN9Nj"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = text.replace('(ap)', '')\n",
        "    text = re.sub(r\"\\'s\", \" is \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r\"\\\\\", \"\", text)\n",
        "    text = re.sub(r\"\\'\", \"\", text)\n",
        "    text = re.sub(r\"\\\"\", \"\", text)\n",
        "    text = re.sub('[^a-zA-Z ?!]+', '', text)\n",
        "    text = _removeNonAscii(text)\n",
        "    text = text.strip()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDg2YrQ7N9Nk"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(word_tokens):\n",
        "    filtered_sentence = []\n",
        "    stop_words = stopwords.words('english')\n",
        "    specific_words_list = ['char', 'u', 'hindustan', 'doj', 'washington']\n",
        "    stop_words.extend(specific_words_list)\n",
        "    for w in word_tokens:\n",
        "        if w not in stop_words:\n",
        "            filtered_sentence.append(w)\n",
        "    return filtered_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4HnnrsiN9Nk"
      },
      "outputs": [],
      "source": [
        "def lemmatize(x):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-lwf6tuN9Nk"
      },
      "outputs": [],
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CASnFpj9N9Nk"
      },
      "outputs": [],
      "source": [
        "def tokenize(x):\n",
        "    return tokenizer.tokenize(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdQ_hWw_N9Nl"
      },
      "outputs": [],
      "source": [
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amt8cwhNjU_9"
      },
      "outputs": [],
      "source": [
        "def sentiment_analysis(prod):\n",
        "    prod['combined_text'] = prod['title'].map(str)\n",
        "\n",
        "    # applying all of these functions to the our dataframe\n",
        "    prod['combined_text'] = prod['combined_text'].map(clean_text)\n",
        "    prod['tokens'] = prod['combined_text'].map(tokenize)\n",
        "    prod['tokens'] = prod['tokens'].map(remove_stopwords)\n",
        "    prod['lems'] = prod['tokens'].map(lemmatize)\n",
        "    sia = SIA()\n",
        "    results = []\n",
        "    for line in prod['lems']:\n",
        "        pol_score = sia.polarity_scores(line)\n",
        "        pol_score['lems'] = line\n",
        "        results.append(pol_score)\n",
        "    headlines_polarity = pd.DataFrame.from_records(results)\n",
        "    temp = []\n",
        "    # for line in prod['branch']:\n",
        "        # temp.append(line)\n",
        "    # headlines_polarity['branch'] = temp\n",
        "    headlines_polarity['label'] = 0\n",
        "    headlines_polarity.loc[headlines_polarity['compound'] > 0.2, 'label'] = 1\n",
        "    headlines_polarity.loc[headlines_polarity['compound'] < -0.2, 'label'] = -1\n",
        "    headlines_polarity['word_count'] = headlines_polarity['lems'].apply(lambda x: len(str(x).split()))\n",
        "    headlines_polarity.head()\n",
        "    # gk = headlines_polarity.groupby(['branch', 'label'])\n",
        "    # fk = headlines_polarity.groupby('branch')['compound'].mean()\n",
        "    # fk = fk.to_frame()\n",
        "    result = [prod, headlines_polarity]\n",
        "    headlines_polarity = headlines_polarity.rename_axis(index=None)\n",
        "    return pd.merge(prod, headlines_polarity, on=[\"lems\"], how=\"left\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2edY3CuN9Nl"
      },
      "outputs": [],
      "source": [
        "from datetime import date\n",
        "\n",
        "def outsource_news():\n",
        "    googleNewsByStreet()\n",
        "    prod = pd.concat(final)\n",
        "    prod = prod.drop_duplicates('title', keep='first')\n",
        "    print(prod)\n",
        "    status_val.append(30)\n",
        "\n",
        "    final_prod = sentiment_analysis(prod)\n",
        "\n",
        "    # mail_data(final_prod) & upload_data_complete(final_prod)\n",
        "\n",
        "    final_prod = final_prod.replace(np.nan,'',regex=True)\n",
        "\n",
        "    # forecast_keywords = ['sale', 'sport', 'beverage', 'retail', 'vendor', 'market', 'morrisons', 'tesco', 'coles', 'business', 'shopping', 'weather',\n",
        "    #                      'parties', 'events', 'walmart']\n",
        "\n",
        "    second_keywords = ['bank holiday', 'heatwave', 'inflation', 'street party', 'rainfall', 'snow', 'retail', 'beverage', 'tesco', 'walmart', 'morrisons', 'weather',\n",
        "                       'brc', 'mothers day', 'new store launch', 'lidl', 'homebase', 'walmart', 'new tesco store', 'coles', 'supermarket', 'shoppers', 'store', 'grocery', 'strike', 'holiday'\n",
        "                       'shops', 'markets','holiday', 'lockdown','grocery sales', 'carnival', 'festival', 'party', \"sainsbury\", \"supply chain\", \"flood\", \"wendys\",\n",
        "                       'ocado', 'spencer', 'asda']\n",
        "\n",
        "    remove_keywords = ['accident', 'incident', 'injury', 'political', 'police', 'death', 'traffic', 'lord', 'war', 'actor', 'movie', 'star', 'lord', 'sex', 'gay',\n",
        "                       'fight', 'crash', 'life', 'plans', 'weapons', 'dating', 'radio', 'tv', 'guinness', 'husband', 'fashion', 'attack']\n",
        "\n",
        "    store_keywords = ['opens', 'closes', 'closed', 'opened', 'open', 'close',\n",
        "                      'shut', 'confining', 'unopen', 'opening',\n",
        "                      'close down', 'closing', 'shut down', 'conclude', 'ending', 'shutdown', 'closedown',\n",
        "                      'closure', 'temporary', 'extended', 'shutting', 'launch', 'shuts', 'closures']\n",
        "\n",
        "    store_remove_keywords = ['ftse', 'pubs', 'pub', 'life', 'stocks', 'earnings', 'dining', 'restaurants', 'stock', 'rocket', 'fashion', 'restaurant',\n",
        "                             'letter', 'bills', 'investment', 'childrenswear', 'blizzard', 'infamous', 'qualifying', 'sports', 'bar', 'cafe',\n",
        "                             'technology', 'dental', 'boobs', 'school','plans', 'flixbus', 'allegations', 'pharmacy', 'attack', 'driver', 'fitness', 'students',\n",
        "                             'charities']\n",
        "\n",
        "    competitor_keywords = ['tesco', 'wendys', 'lidl', 'sainsburys', 'sainsbury', 'aldi', 'morrisons', 'spencer', 'asda', 'supermarket',\n",
        "                            'co', 'ocado', 'sparks', 'b&m', 'iceland', 'waitrose']\n",
        "\n",
        "    print(final_prod)\n",
        "\n",
        "    for index, row in final_prod.iterrows():\n",
        "      if (len(np.intersect1d(row['tokens'], store_keywords)) == 0):\n",
        "        # if(len(np.intersect1d(row['tokens'], competitor_keywords)) == 0):\n",
        "        final_prod.drop(index=index, axis=0, inplace=True)\n",
        "      else:\n",
        "        if(len(np.intersect1d(row['tokens'], competitor_keywords)) == 0):\n",
        "          final_prod.drop(index=index, axis=0, inplace=True)\n",
        "\n",
        "    for index, row in final_prod.iterrows():\n",
        "      for value in row['tokens']:\n",
        "        val = value.capitalize()\n",
        "        try:\n",
        "            final_prod.at[index,'bu_num'] = branch_keyword_bu_num[val]\n",
        "            final_prod.at[index,'branch'] = val\n",
        "        except:\n",
        "            n = 0\n",
        "\n",
        "    final_prod = final_prod.drop_duplicates('title', keep='first')\n",
        "    final_prod = final_prod.drop_duplicates('lems', keep='first')\n",
        "    final_prod = final_prod.drop_duplicates('tokens', keep='first')\n",
        "\n",
        "    final_prod['title'] = final_prod['title'].astype(str)\n",
        "\n",
        "    final_prod['competitor_evt_indchar'] = ['Yes' if(len(np.intersect1d(x,competitor_keywords)) > 0) else 'No' for x in final_prod['tokens']]\n",
        "\n",
        "    counter_guid = int(date.today().strftime(\"%Y%m%d\"))\n",
        "    final_prod['efsevt_guid'] = [(counter_guid*1000)+i for i in range(len(final_prod))]\n",
        "\n",
        "    print(final_prod.dtypes)\n",
        "    print(final_prod_events.dtypes)\n",
        "\n",
        "    foriegn_key = []\n",
        "\n",
        "    for index, row in final_prod.iterrows():\n",
        "      flag = False\n",
        "      for index_event, row_event in final_prod_events.iterrows():\n",
        "        if(row['keyword'] != '' ):\n",
        "          if(row['keyword'] in row_event['NAME']):\n",
        "            print(row['keyword'],row_event['NAME'])\n",
        "            foriegn_key.append(row_event['GUID'])\n",
        "            flag = True\n",
        "      if(flag == False):\n",
        "        foriegn_key.append(0)\n",
        "\n",
        "    print(foriegn_key)\n",
        "\n",
        "    final_prod['guid'] = [(counter_guid*2000)+i for i in range(len(final_prod))]\n",
        "    final_prod['fixed_annual_ind'] = 'n'\n",
        "    final_prod['perm_env_ind'] = 'n'\n",
        "    final_prod['cancelled_ind'] = 'n'\n",
        "    final_prod['create_user'] = ''\n",
        "    final_prod['update_user'] = ''\n",
        "    final_prod['perm_env_ind'] = 'n'\n",
        "    final_prod['crt_timestamp'] = date.today()\n",
        "    final_prod['upd_timestamp'] = date.today()\n",
        "\n",
        "\n",
        "    final_prod.rename(columns = {'link':'source_of_event'}, inplace = True)\n",
        "    final_prod[[\"datetime\"]] = final_prod[[\"datetime\"]].astype(str)\n",
        "    final_prod.columns = final_prod.columns.str.upper()\n",
        "\n",
        "    final_prod.to_csv('Events.csv', mode='a', index=False, header=False)\n",
        "    return final_prod\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBR08rg-jxHV"
      },
      "outputs": [],
      "source": [
        "!pip install geopy\n",
        "!pip install pgeocode\n",
        "\n",
        "from geopy.geocoders import Photon, GoogleV3, Nominatim\n",
        "import pgeocode\n",
        "from math import cos, asin, sqrt, pi\n",
        "\n",
        "def distance(lat1, lon1, lat2, lon2):\n",
        "    p = pi/180\n",
        "    a = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p) * cos(lat2*p) * (1-cos((lon2-lon1)*p))/2\n",
        "    return round(12742 * asin(sqrt(a)),2)\n",
        "\n",
        "\n",
        "def place_distance(string1,string2):\n",
        "    try:\n",
        "      geolocator_addr = Nominatim(user_agent=\"efs\")\n",
        "      # place = \"Lidl,Bath\"\n",
        "      # place_2 = \"Waitrose,Bath\"\n",
        "      place = string1 + \",\" + string2\n",
        "      place_2 = \"Waitrose,\" + string2\n",
        "      # location = geolocator.geocode(place)\n",
        "      pin = geolocator_addr.geocode(place)\n",
        "      pin_2 = geolocator_addr.geocode(place_2)\n",
        "      # print(location)\n",
        "      print(pin)\n",
        "      print(pin_2)\n",
        "      print(pin.raw['lat'],pin.raw['lon'],pin_2.raw['lat'],pin_2.raw['lon'])\n",
        "    except:\n",
        "      return 'N/A'\n",
        "\n",
        "    return distance(float(pin.raw['lat']),float(pin.raw['lon']),float(pin_2.raw['lat']),float(pin_2.raw['lon']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRLgOI_63mMN"
      },
      "outputs": [],
      "source": [
        "road = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32NQzwC3sQUV"
      },
      "outputs": [],
      "source": [
        "!pip install requests lxml\n",
        "!pip install beautifulsoup4\n",
        "!pip install geopy\n",
        "!pip install pgeocode\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from geopy.geocoders import Photon, GoogleV3, Nominatim\n",
        "import pgeocode\n",
        "from math import cos, asin, sqrt, pi\n",
        "import re\n",
        "import math\n",
        "\n",
        "def distance_infrastructure(lat1, lon1, lat2, lon2):\n",
        "    p = pi/180\n",
        "    a = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p) * cos(lat2*p) * (1-cos((lon2-lon1)*p))/2\n",
        "    return round(12742 * asin(sqrt(a)),2)\n",
        "\n",
        "\n",
        "def place_distance_infrastructure(string2, work):\n",
        "    temp = []\n",
        "    try:\n",
        "      route_df = pd.read_csv(\"Branch_Lat_Lon.csv\")\n",
        "      geolocator_addr = Nominatim(user_agent=\"http\")\n",
        "      place_2 = string2 + \", London, UK\"\n",
        "      pin_2 = geolocator_addr.geocode(place_2)\n",
        "      print(pin_2)\n",
        "      print(pin_2.raw['lat'],pin_2.raw['lon'])\n",
        "      actual = 999999\n",
        "      if(pin_2 != 'None'):\n",
        "        for index, row in route_df.iterrows():\n",
        "          dif = distance_infrastructure(float(pin_2.raw['lat']),float(pin_2.raw['lon']),row[\"lat\"],row[\"lon\"])\n",
        "          if( dif < 20):\n",
        "            if(actual > dif):\n",
        "              actual = dif\n",
        "              branch_name = row[\"branch\"]\n",
        "        if(actual <= 1):\n",
        "          print(actual)\n",
        "          temp.append(branch_name)\n",
        "          print(branch_name)\n",
        "          print(\"Yes\")\n",
        "          road.append([ string2 + \" \" + work, branch_name, \"\", date.today(), branch_keyword_bu_num[branch_name], actual,\"\"])\n",
        "          return branch_name\n",
        "        print(actual)\n",
        "    except:\n",
        "      n = 0\n",
        "    return None\n",
        "\n",
        "\n",
        "def infrastructure():\n",
        "  url = \"https://tfl.gov.uk/traffic/status/?Input=&lineIds=&dateTypeSelect=Future%20date&direction=&startDate=\"+date.today().strftime(\"%Y-%m-%d\")+\"T00%3A00%3A00&endDate=\"+date.today().strftime(\"%Y-%m-%d\")+\"T23%3A59%3A59&lat=51.50721740722656&lng=-0.12758620083332062&placeType=stoppoint&input=London%2C%20UK\"\n",
        "  # url = \"https://tfl.gov.uk/traffic/status/?Input=England%2C%20UK&lineIds=&dateTypeSelect=Future%20date&direction=&startDate=2023-06-23T00%3A00%3A00&endDate=2023-06-23T23%3A59%3A59&lat=52.35551834106445&lng=-1.1743197441101074&placeType=placeextra&input=london%2C%20uk\"\n",
        "  # Send a GET request to the website\n",
        "  resp = requests.get(url)\n",
        "  soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "  ele = soup.select('div[class^=\\\"road-disruption\\\"]')\n",
        "  # print(ele[0].text)\n",
        "  street = []\n",
        "  works = []\n",
        "  for element in ele:\n",
        "      h2_tags = element.select('h4')\n",
        "      p_tags = element.select('p[class^=\\\"topmargin\\\"]')\n",
        "      date_tags = element.select('p[class^=\\\"highlight dates\\\"]')\n",
        "      print(date_tags[0].text.strip(\"\\n\\n\").split(\"\\n\"))\n",
        "      for h2_tag,p_tag in zip(h2_tags,p_tags):\n",
        "          # print(h2_tag.text.strip().split(\" \"))\n",
        "          if(\"Works\" in p_tag.text):\n",
        "            arr = h2_tag.text.strip().split(\" \")\n",
        "            word = \"\"\n",
        "            for i in range(1,len(arr)-1):\n",
        "              if('(' not in arr[i]):\n",
        "                word = word + \" \" + arr[i]\n",
        "            # word = word + \" \" +arr[-1]\n",
        "            street.append(word)\n",
        "            works.append(p_tag.text)\n",
        "\n",
        "  street = list(set(street))\n",
        "  print(len(street))\n",
        "  # print(street)\n",
        "\n",
        "  for i,j in zip(street,works):\n",
        "    place_distance_infrastructure(i,j)\n",
        "\n",
        "  return road\n",
        "\n",
        "\n",
        "# place_distance_infrastructure(\"Lea Gate, Blackpool Road, Preston\", \"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFBGJI7DBYMD"
      },
      "outputs": [],
      "source": [
        "!pip install pretty-html-table\n",
        "!pip install pyshorteners\n",
        "!pip install xlsxwriter\n",
        "\n",
        "import smtplib, ssl\n",
        "from smtplib import SMTP\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "import pandas as pd\n",
        "from pretty_html_table import build_table\n",
        "from pyshorteners import Shortener\n",
        "from io import StringIO, BytesIO\n",
        "from email.mime.application import MIMEApplication\n",
        "from datetime import date, timedelta\n",
        "import xlsxwriter\n",
        "from cryptography.fernet import Fernet\n",
        "\n",
        "def mail_data(final_prod, final_prod_old):\n",
        "  port = 465  # For SSL\n",
        "  context = ssl.create_default_context()\n",
        "  mail_df = pd.DataFrame()\n",
        "\n",
        "  mail_df[\"TITLE\"] = final_prod[\"TITLE\"]\n",
        "  mail_df[\"BRANCH\"] = final_prod[\"BRANCH\"]\n",
        "  mail_df[\"SOURCE\"] = final_prod[\"MEDIA\"]\n",
        "  mail_df[\"DATETIME\"] = final_prod[\"DATETIME\"]\n",
        "  mail_df[\"BRANCH_NUM\"] = final_prod[\"BU_NUM\"]\n",
        "\n",
        "  distance_arr = []\n",
        "  for index, row in final_prod.iterrows():\n",
        "    if(row[\"KEYWORD\"] and row[\"BRANCH\"]):\n",
        "        distance_arr.append(place_distance(row[\"KEYWORD\"], row[\"BRANCH\"]))\n",
        "  mail_df[\"DISTANCE IN MILES\"] = distance_arr\n",
        "\n",
        "  urls = []\n",
        "  for index,row in final_prod.iterrows():\n",
        "      x = Shortener().tinyurl.short(row[\"SOURCE_OF_EVENT\"])\n",
        "      urls.append(x)\n",
        "\n",
        "  mail_df[\"LINK\"] = urls\n",
        "\n",
        "  for index, row in mail_df.iterrows():\n",
        "    if(row[\"DISTANCE IN MILES\"] != 'N/A'):\n",
        "      if(row[\"DISTANCE IN MILES\"] > 25):\n",
        "          mail_df.drop(index=index, axis=0, inplace=True)\n",
        "\n",
        "  all_prod = pd.concat([mail_df, final_prod_old])\n",
        "  road = infrastructure()\n",
        "  print(road)\n",
        "  road_df = pd.DataFrame(road, columns = [\"TITLE\", \"BRANCH\", \"SOURCE\", \"DATETIME\", \"BRANCH_NUM\", \"DISTANCE IN MILES\", \"LINK\"])\n",
        "  road_df = road_df.drop_duplicates(\"BRANCH\", keep=\"first\")\n",
        "\n",
        "  html_table = mail_df.to_html(index=False, classes='example-table')\n",
        "  road_table = road_df.to_html(index=False, classes='example-table')\n",
        "\n",
        "  text = f\"Hello Alex and Tim,\\n Herewith attaching the events captured for all the competitors (core event types) including all the branches from \"+ (date.today() - timedelta(days = 1)).strftime(\"%d-%m-%Y\") +\" to \" + date.today().strftime(\"%d-%m-%Y\") + \" which are auto-generated from the script.\\n\\n\\nThanks And Regards,\\nSubhash\\n\\n\\n\"\n",
        "\n",
        "  print(mail_df)\n",
        "\n",
        "  html_table = html_table.replace('<th>', '<th style=\"padding: 10px 90px 10px 90px;\">', 1)\n",
        "  road_table = road_table.replace('<th>', '<th style=\"padding: 10px 80px 10px 80px;\">', 1)\n",
        "\n",
        "  if(mail_df.empty):\n",
        "    html_table\n",
        "\n",
        "# HTML Styling\n",
        "  html = f'''\n",
        "<html>\n",
        "<head>\n",
        "    <style>\n",
        "        table.example-table th{{\n",
        "              padding: 10px;\n",
        "              text-align: center;\n",
        "              background-color: #FFFFFF;\n",
        "              font-weight: bold;\n",
        "              font-size: 14px;\n",
        "              width: 400px;\n",
        "          }}\n",
        "\n",
        "          table.example-table th:first-child {{\n",
        "            padding: 20px 100px 20px 100px; /* Set the desired width for the fourth column */\n",
        "          }}\n",
        "\n",
        "          table.example-table td {{\n",
        "            padding: 5px;\n",
        "            color: black;\n",
        "            font-size: 12px;\n",
        "            width: 400px;\n",
        "            font-family: Century Gothic, sans-serif;\n",
        "          }}\n",
        "\n",
        "        /* Add custom styles here */\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "      <pre>{text}</pre>\n",
        "        {html_table}\n",
        "        <br/>\n",
        "        <br/>\n",
        "        {road_table}\n",
        "</body>\n",
        "</html>\n",
        "'''\n",
        "\n",
        "  part1 = MIMEText(html, 'html')\n",
        "  msg = MIMEMultipart(\"alternative\")\n",
        "  msg['Subject'] = \"Automated Event Capturing Model\"\n",
        "  recipients = ['subhash.verma@johnlewis.co.uk']\n",
        "  # recipients = ['mitali.patel@johnlewis.co.uk']\n",
        "  msg['To'] = \", \".join(recipients)\n",
        "  msg.attach(part1)\n",
        "\n",
        "  file_name = date.today().strftime(\"%d-%m-%Y\") + \"_Events.xlsx\"\n",
        "  # output = io.BytesIO()\n",
        "  textStream = BytesIO()\n",
        "  writer = pd.ExcelWriter(textStream, engine='xlsxwriter')\n",
        "  all_prod.to_excel(writer,sheet_name=\"Competitor Events\",index=False)\n",
        "  road_df.to_excel(writer,sheet_name=\"Road Closure Events\",index=False)\n",
        "  writer.close()\n",
        "  textStream.seek(0)\n",
        "  attachment = MIMEApplication(textStream.read(), name= file_name)\n",
        "  attachment['Content-Disposition'] = 'attachment; filename=\"{}\"'.format(file_name)\n",
        "  msg.attach(attachment)\n",
        "\n",
        "  with smtplib.SMTP_SSL(\"smtp.gmail.com\", port, context=context) as server:\n",
        "      dec = str(Fernet('egupkHT3QJHG1c5dcPGiWEZaWdH04_uhgyD-8lYNxWM=').decrypt(b'gAAAAABpHy2IRPVaNZJU3a2jDD68rGtj0jMYEvJyrWRJepy-wUXuHwKdmAzMTSDXAWkP4S8tUWCd6Q5egqHWKGFkMx18sIu6NUPerPx9TSkeFpCedLP3LAc='), 'UTF-8')\n",
        "      print(\"IN\")\n",
        "      server.login(\"subhash.verma@johnlewis.co.uk\", dec)\n",
        "      server.sendmail(\"subhash.verma@johnlewis.co.uk\", recipients, msg.as_string())\n",
        "\n",
        "  return all_prod, road_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3FYrkChWAUo"
      },
      "outputs": [],
      "source": [
        "import gspread\n",
        "if __name__ == '__main__':\n",
        "  final_prod = outsource_news()\n",
        "  #gc = gspread.service_account(filename='service_account.json')\n",
        "  #worksheet_old = gc.open(\"05-08-2023_Events\").sheet1\n",
        "  #worksheet_old_2 = gc.open(\"05-08-2023_Events\").get_worksheet(1)\n",
        "  #worksheet_old_2.clear()\n",
        "  #final_prod = outsource_news()\n",
        "  #url = \"https://docs.google.com/spreadsheets/d/1N-ql7D4kV-qfHzSN8YCpQw1oZVF_hWUcZHajsC1qU4s/export?format=xlsx\"\n",
        "  #df = pd.read_excel(url1, sheet_name = \"Sheet1\")\n",
        "  final_prod_old = pd.DataFrame()\n",
        "  #print(final_prod)\n",
        "  #print(final_prod_old)\n",
        "  #print(df)\n",
        "  all_prod, road_df = mail_data(final_prod, final_prod_old)\n",
        "  #worksheet_old.clear()\n",
        "  #set_with_dataframe(worksheet_old, all_prod)\n",
        "  #set_with_dataframe(worksheet_old_2, road_df)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
