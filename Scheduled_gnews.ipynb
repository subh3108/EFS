{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.x",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "file_extension": ".py"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scheduled GNews — Resilient Pipeline (Corrected)\\n",
        "\\n",
        "This notebook fixes empty DataFrame crashes by:\\n",
        "- Using a resilient fetcher (GoogleNews with fallback to gnewsclient).\\n",
        "- Guarding concatenations/returns to avoid \\\"No objects to concatenate\\\".\\n",
        "- Vectorized filtering + deduplication.\\n",
        "- Reusing your globals when available.\\n",
        "\\n",
        "Run the **Run & Preview** cell at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===========================================\\n",
        "# Corrected resilient pipeline (single cell)\\n",
        "# ===========================================\\n",
        "# If needed (first kernel run):\\n",
        "# !pip install GoogleNews\\n",
        "# !pip install gnewsclient\\n",
        "# !pip install nltk pandas numpy\\n",
        "\\n",
        "import time\\n",
        "import re\\n",
        "import numpy as np\\n",
        "import pandas as pd\\n",
        "from datetime import date\\n",
        "from IPython.display import display\\n",
        "\\n",
        "# News packages\\n",
        "from GoogleNews import GoogleNews\\n",
        "from gnewsclient import gnewsclient\\n",
        "\\n",
        "# NLP packages\\n",
        "import nltk\\n",
        "from nltk.tokenize import RegexpTokenizer\\n",
        "from nltk.corpus import stopwords\\n",
        "from nltk.stem import WordNetLemmatizer\\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\\n",
        "\\n",
        "# Download only the required NLTK resources\\n",
        "nltk.download('punkt', quiet=True)\\n",
        "nltk.download('stopwords', quiet=True)\\n",
        "nltk.download('wordnet', quiet=True)\\n",
        "nltk.download('vader_lexicon', quiet=True)\\n",
        "\\n",
        "# ---------- CONFIG GUARDS (reuse your existing globals if already defined) ----------\\n",
        "try:\\n",
        "    branch_keyword_bu_num\\n",
        "except NameError:\\n",
        "    branch_keyword_bu_num = {\\n",
        "        'London': 101,\\n",
        "        'Manchester': 102,\\n",
        "        'Birmingham': 103,\\n",
        "        'Leeds': 104\\n",
        "    }\\n",
        "\\n",
        "try:\\n",
        "    branch_keyword\\n",
        "except NameError:\\n",
        "    branch_keyword = list(branch_keyword_bu_num.keys())\\n",
        "\\n",
        "try:\\n",
        "    keywords\\n",
        "except NameError:\\n",
        "    keywords = ['Lidl', 'Waitrose', 'Tesco', 'Walmart', \"Sainsbury\\'s\", 'Aldi', 'Asda', 'Marks & Spencers', \"Morrison\\'s\"]\\n",
        "\\n",
        "try:\\n",
        "    final_prod_events\\n",
        "except NameError:\\n",
        "    final_prod_events = pd.DataFrame()\\n",
        "\\n",
        "try:\\n",
        "    final\\n",
        "except NameError:\\n",
        "    final = []\\n",
        "\\n",
        "# ===========================================\\n",
        "# HELPERS\\n",
        "# ===========================================\\n",
        "def _safe_drop(df: pd.DataFrame, cols):\\n",
        "    cols_present = [c for c in cols if c in df.columns]\\n",
        "    return df.drop(columns=cols_present) if cols_present else df\\n",
        "\\n",
        "def _removeNonAscii(s: str) -> str:\\n",
        "    return ''.join(i for i in s if ord(i) < 128)\\n",
        "\\n",
        "def clean_text(text: str) -> str:\\n",
        "    text = str(text).lower()\\n",
        "    text = re.sub(r\"what's\", 'what is ', text)\\n",
        "    text = text.replace('(ap)', '')\\n",
        "    text = re.sub(r\"\\'s\", ' is ', text)\\n",
        "    text = re.sub(r\"\\'ve\", ' have ', text)\\n",
        "    text = re.sub(r\"can't\", 'cannot ', text)\\n",
        "    text = re.sub(r\"n't\", ' not ', text)\\n",
        "    text = re.sub(r\"i'm\", 'i am ', text)\\n",
        "    text = re.sub(r\"\\'re\", ' are ', text)\\n",
        "    text = re.sub(r\"\\'d\", ' would ', text)\\n",
        "    text = re.sub(r\"\\'ll\", ' will ', text)\\n",
        "    text = re.sub(r'\\\\W+', ' ', text)\\n",
        "    text = re.sub(r'\\\\s+', ' ', text)\\n",
        "    text = text.replace('\\\\\\\\', '')\\n",
        "    text = text.replace('\\\"', '')\\n",
        "    text = text.replace(\"'\", '')\\n",
        "    text = re.sub('[^a-zA-Z ?!]+', ' ', text)\\n",
        "    text = _removeNonAscii(text)\\n",
        "    return text.strip()\\n",
        "\\n",
        "tokenizer = RegexpTokenizer(r'\\\\w+')\\n",
        "def tokenize(x: str):\\n",
        "    return tokenizer.tokenize(x)\\n",
        "\\n",
        "def remove_stopwords(word_tokens):\\n",
        "    sw = set(stopwords.words('english'))\\n",
        "    specific_words_list = {'char', 'u', 'hindustan', 'doj', 'washington'}\\n",
        "    sw |= specific_words_list\\n",
        "    return [w for w in word_tokens if w not in sw]\\n",
        "\\n",
        "def lemmatize(tokens):\\n",
        "    lemmatizer = WordNetLemmatizer()\\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in tokens])\\n",
        "\\n",
        "def fetch_news(query, period='7d', lang='en'):\\n",
        "    \"\"\"\\n",
        "    Tries GoogleNews first (search+results); if empty, falls back to gnewsclient.\\n",
        "    Returns list[dict] with keys: title, media, date, datetime, link, desc.\\n",
        "    \"\"\"\\n",
        "    results = []\\n",
        "    # ---- Try GoogleNews ----\\n",
        "    try:\\n",
        "        gn = GoogleNews(lang=lang, period=period)\\n",
        "        gn.search(query)\\n",
        "        results = gn.results() if hasattr(gn, 'results') else []\\n",
        "        if results is None:\\n",
        "            results = []\\n",
        "    except Exception:\\n",
        "        results = []\\n",
        "\\n",
        "    # ---- Fallback to gnewsclient if empty ----\\n",
        "    if not results:\\n",
        "        try:\\n",
        "            client = gnewsclient.NewsClient(\\n",
        "                language='english',\\n",
        "                location='United Kingdom',\\n",
        "                topic='Business',\\n",
        "                max_results=12,\\n",
        "                query=query\\n",
        "            )\\n",
        "            gres = client.get_news() or []\\n",
        "            mapped = []\\n",
        "            for item in gres:\\n",
        "                mapped.append({\\n",
        "                    'title': item.get('title'),\\n",
        "                    'media': item.get('publisher') or item.get('source'),\\n",
        "                    'date': item.get('published'),\\n",
        "                    'datetime': item.get('published'),\\n",
        "                    'link': item.get('link'),\\n",
        "                    'desc': item.get('description') or ''\\n",
        "                })\\n",
        "            results = mapped\\n",
        "        except Exception:\\n",
        "            results = []\\n",
        "    return results\\n",
        "\\n",
        "# ===========================================\\n",
        "# SENTIMENT (close to your original)\\n",
        "# ===========================================\\n",
        "def sentiment_analysis(prod: pd.DataFrame) -> pd.DataFrame:\\n",
        "    prod = prod.copy()\\n",
        "    prod['combined_text'] = prod['title'].map(str)\\n",
        "    prod['combined_text'] = prod['combined_text'].map(clean_text)\\n",
        "    prod['tokens'] = prod['combined_text'].map(tokenize)\\n",
        "    prod['tokens'] = prod['tokens'].map(remove_stopwords)\\n",
        "    prod['lems'] = prod['tokens'].map(lemmatize)\\n",
        "\\n",
        "    sia = SIA()\\n",
        "    pol_records = []\\n",
        "    for line in prod['lems']:\\n",
        "        pol_score = sia.polarity_scores(line)\\n",
        "        pol_score['lems'] = line\\n",
        "        pol_records.append(pol_score)\\n",
        "    headlines_polarity = pd.DataFrame.from_records(pol_records)\\n",
        "\\n",
        "    headlines_polarity['label'] = 0\\n",
        "    headlines_polarity.loc[headlines_polarity['compound'] > 0.2, 'label'] = 1\\n",
        "    headlines_polarity.loc[headlines_polarity['compound'] < -0.2, 'label'] = -1\\n",
        "    headlines_polarity['word_count'] = headlines_polarity['lems'].apply(lambda x: len(str(x).split()))\\n",
        "\\n",
        "    merged = pd.merge(prod, headlines_polarity, on=['lems'], how='left')\\n",
        "    return merged\\n",
        "\\n",
        "# ===========================================\\n",
        "# CORE NEWS GATHER + FILTER\\n",
        "# ===========================================\\n",
        "def googleNewsByStreet(period='7d', sleep_sec=0.9, lang='en'):\\n",
        "    \"\"\"\\n",
        "    Builds consolidated DataFrame of news for every (branch, keyword) pair.\\n",
        "    Appends non-empty results to the global 'final' list, and also returns the DataFrame.\\n",
        "    \"\"\"\\n",
        "    global final\\n",
        "    frames = []\\n",
        "\\n",
        "    for branch in branch_keyword:\\n",
        "        bu_num_val = branch_keyword_bu_num.get(branch, None)\\n",
        "        for keyword in keywords:\\n",
        "            query = f\"{branch} {keyword}\"\\n",
        "            results = fetch_news(query, period=period, lang=lang)\\n",
        "            if not results:\\n",
        "                continue\\n",
        "            df = pd.DataFrame(results)\\n",
        "            if df.empty:\\n",
        "                continue\\n",
        "            df['keyword'] = keyword\\n",
        "            df['branch']  = branch\\n",
        "            df['bu_num']  = bu_num_val\\n",
        "            frames.append(df)\\n",
        "            time.sleep(sleep_sec)\\n",
        "\\n",
        "    if not frames:\\n",
        "        try:\\n",
        "            if isinstance(final, list):\\n",
        "                final.clear()\\n",
        "        except NameError:\\n",
        "            final = []\\n",
        "        return pd.DataFrame(columns=['title','media','date','datetime','link','desc','keyword','branch','bu_num'])\\n",
        "\\n",
        "    data = pd.concat(frames, ignore_index=True)\\n",
        "    data = _safe_drop(data, ['img', 'site'])\\n",
        "    if {'title','link'}.issubset(data.columns):\\n",
        "        data = data.drop_duplicates(subset=['title', 'link'])\\n",
        "    if isinstance(final, list):\\n",
        "        final.append(data.copy())\\n",
        "    return data\\n",
        "\\n",
        "def outsource_news():\\n",
        "    \"\"\"\\n",
        "    End-to-end builder:\\n",
        "      * gathers news via googleNewsByStreet\\n",
        "      * runs sentiment_analysis\\n",
        "      * filters for store + competitor keywords (vectorized)\\n",
        "      * enriches with GUIDs and standard columns\\n",
        "      * returns final_prod (ready for export / email)\\n",
        "    \"\"\"\\n",
        "    prod_candidate = googleNewsByStreet(period='7d')\\n",
        "    frames = final if isinstance(final, list) else []\\n",
        "\\n",
        "    if prod_candidate is not None and not prod_candidate.empty:\\n",
        "        prod = prod_candidate\\n",
        "    elif frames:\\n",
        "        prod = pd.concat(frames, ignore_index=True)\\n",
        "    else:\\n",
        "        cols = ['title','media','date','datetime','link','desc','keyword','branch','bu_num']\\n",
        "        return pd.DataFrame(columns=cols)\\n",
        "\\n",
        "    prod = prod.replace(np.nan, '', regex=True)\\n",
        "    prod = prod.drop_duplicates('title', keep='first')\\n",
        "\\n",
        "    final_prod = sentiment_analysis(prod)\\n",
        "\\n",
        "    store_keywords = [\\n",
        "        'opens', 'closes', 'closed', 'opened', 'open', 'close', 'shut', 'confining',\\n",
        "        'unopen', 'opening', 'close down', 'closing', 'shut down', 'conclude', 'ending',\\n",
        "        'shutdown', 'closedown', 'closure', 'temporary', 'extended', 'shutting',\\n",
        "        'launch', 'shuts', 'closures'\\n",
        "    ]\\n",
        "\\n",
        "    competitor_keywords = [\\n",
        "        'tesco', 'wendys', 'lidl', 'sainsburys', 'sainsbury', 'aldi', 'morrisons',\\n",
        "        'spencer', 'asda', 'supermarket', 'co', 'ocado', 'b&m', 'iceland', 'waitrose'\\n",
        "    ]\\n",
        "\\n",
        "    def _match_any(tokens, vocab):\\n",
        "        if isinstance(tokens, str):\\n",
        "            tokens = tokens.split()\\n",
        "        return len(np.intersect1d(tokens, vocab)) > 0\\n",
        "\\n",
        "    mask_store = final_prod['tokens'].apply(lambda x: _match_any(x, store_keywords))\\n",
        "    mask_comp  = final_prod['tokens'].apply(lambda x: _match_any(x, competitor_keywords))\\n",
        "    final_prod = final_prod[mask_store & mask_comp].copy()\\n",
        "\\n",
        "    bk_map = {k.lower(): (k, v) for k, v in branch_keyword_bu_num.items()}\\n",
        "\\n",
        "    def infer_branch_info(tokens):\\n",
        "        if isinstance(tokens, str):\\n",
        "            tokens = tokens.split()\\n",
        "        for t in tokens:\\n",
        "            key = t.lower().replace('’', \"'\")\\n",
        "            if key in bk_map:\\n",
        "                return bk_map[key]\\n",
        "        return (np.nan, np.nan)\\n",
        "\\n",
        "    if 'branch' not in final_prod.columns:\\n",
        "        final_prod['branch'] = ''\\n",
        "    if 'bu_num' not in final_prod.columns:\\n",
        "        final_prod['bu_num'] = np.nan\\n",
        "\\n",
        "    missing_branch = ~final_prod['branch'].astype(str).str.len().astype(bool)\\n",
        "    branch_bu = final_prod.loc[missing_branch, 'tokens'].apply(infer_branch_info)\\n",
        "    if not branch_bu.empty:\\n",
        "        final_prod.loc[missing_branch, 'branch'] = branch_bu.apply(lambda x: x[0])\\n",
        "        final_prod.loc[missing_branch, 'bu_num'] = branch_bu.apply(lambda x: x[1])\\n",
        "\\n",
        "    for col in ['title', 'lems', 'tokens']:\\n",
        "        if col in final_prod.columns:\\n",
        "            final_prod = final_prod.drop_duplicates(subset=[col], keep='first')\\n",
        "\\n",
        "    final_prod['title'] = final_prod['title'].astype(str)\\n",
        "    final_prod['competitor_evt_indchar'] = [\\n",
        "        'Yes' if _match_any(x, competitor_keywords) else 'No'\\n",
        "        for x in final_prod['tokens']\\n",
        "    ]\\n",
        "\\n",
        "    counter_guid = int(date.today().strftime('%Y%m%d'))\\n",
        "    final_prod['efsevt_guid'] = [(counter_guid * 1000) + i for i in range(len(final_prod))]\\n",
        "    final_prod['guid'] = [(counter_guid * 2000) + i for i in range(len(final_prod))]\\n",
        "\\n",
        "    # If you maintain final_prod_events, you can attach FK here (0 default)\\n",
        "    foreign_key = []\\n",
        "    if isinstance(final_prod_events, pd.DataFrame) and not final_prod_events.empty:\\n",
        "        for _, row in final_prod.iterrows():\\n",
        "            fk = 0\\n",
        "            kw = str(row.get('keyword', '')).strip()\\n",
        "            if kw:\\n",
        "                for _, evt in final_prod_events.iterrows():\\n",
        "                    if kw in str(evt.get('NAME', '')):\\n",
        "                        fk = evt.get('GUID', 0)\\n",
        "                        break\\n",
        "            foreign_key.append(fk)\\n",
        "    else:\\n",
        "        foreign_key = [0] * len(final_prod)\\n",
        "\\n",
        "    final_prod['fixed_annual_ind'] = 'n'\\n",
        "    final_prod['perm_env_ind'] = 'n'\\n",
        "    final_prod['cancelled_ind'] = 'n'\\n",
        "    final_prod['create_user'] = ''\\n",
        "    final_prod['update_user'] = ''\\n",
        "    final_prod['crt_timestamp'] = date.today()\\n",
        "    final_prod['upd_timestamp'] = date.today()\\n",
        "\\n",
        "    final_prod = final_prod.rename(columns={'link': 'source_of_event'})\\n",
        "    if 'datetime' in final_prod.columns:\\n",
        "        final_prod[['datetime']] = final_prod[['datetime']].astype(str)\\n",
        "\\n",
        "    return final_prod\\n",
        "\\n",
        "# ===========================================\\n",
        "# RUN & PREVIEW\\n",
        "# ===========================================\\n",
        "final_prod = outsource_news()\\n",
        "print('Rows fetched:', len(final_prod))\\n",
        "display(final_prod.head(10))\\n",
        "\\n",
        "# Optional snapshot\\n",
        "# final_prod.to_csv('Events_snapshot.csv', index=False)\\n",
        "\\n",
        "# Optional uppercase columns for downstream\\n",
        "# final_prod_upper = final_prod.copy()\\n",
        "# final_prod_upper.columns = final_prod_upper.columns.str.upper()\\n",
        "# final_prod_upper.to_csv('Events_upper.csv', index=False)\\n"
      ]
    }
  ]
