
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"name":"pip_installs"},
   "outputs": [],
   "source": [
    "# If running in a clean runner, uncomment as needed:\n",
    "!pip install feedparser\n",
    "!pip install beautifulsoup4 lxml\n",
    "!pip install pretty-html-table pyshorteners xlsxwriter\n",
    "!pip install geopy pgeocode\n",
    "!pip install requests lxml\n",
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"name":"imports"},
   "outputs": [],
   "source": [
    "import os, re, math, json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from datetime import datetime, date, timezone, timedelta\n",
    "from string import punctuation\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from rake_nltk import Rake\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "# RSS helper (stable alternative to HTML scrapers)\n",
    "import feedparser\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "# Email & attachments\n",
    "import smtplib, ssl\n",
    "from smtplib import SMTP\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.application import MIMEApplication\n",
    "from pretty_html_table import build_table\n",
    "from pyshorteners import Shortener\n",
    "from io import StringIO, BytesIO\n",
    "import xlsxwriter\n",
    "from cryptography.fernet import Fernet\n",
    "\n",
    "# Geocoding & infra\n",
    "from geopy.geocoders import Photon, GoogleV3, Nominatim\n",
    "import pgeocode\n",
    "from math import cos, asin, sqrt, pi\n",
    "\n",
    "# HTML parsing needed by infrastructure()\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"name":"globals_and_lists"},
   "outputs": [],
   "source": [
    "start_date = []\n",
    "end_date = []\n",
    "\n",
    "sources = [\n",
    "    \"bbc-news\", \"the-telegraph\", \"the-guardian-uk\", \"cnn\", \"abc-news-au\",\n",
    "    \"dailymail.co.uk\", \"metro.co.uk\", \"mirror.co.uk\", \"news.google.com\"\n",
    "]\n",
    "\n",
    "all_keywords = ['strike', 'holiday', 'lockdown', 'inflation', 'grocery sales',\n",
    "                'carnival', 'festival', 'party', 'Walmart', \"Tesco\", \"Sainsbury's\",\n",
    "                \"supply chain\", \"flood\", \"wendys\", \"lidl\"]\n",
    "\n", 
    "keywords = ['Lidl','Waitrose','Tesco','Walmart','Sainsbury\\'s', 'Aldi', 'Asda', 'Marks & Spencers', 'Morrison\\'s']\n",
    "\n",
    "events = ['autumn bank holiday']\n",
    "all_events = ['autumn bank holiday']\n",
    "final_prod_events = pd.DataFrame()\n",
    "counter = 6000\n",
    "gnews_client_topics = ['Top Stories','World','Nation','Business','Technology','Entertainment','Sports','Science','Health']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"name":"branch_dict"},
   "outputs": [],
   "source": [
    "# Replace/expand with your full branch dictionary as needed\n",
    "branch_keyword_bu_num = {\n"
    "'Battersea': 737,\n",
    "'Bagshot Road': 738,\n",
    "'Tubs Hill': 739,\n",
    "'Greenwich': 740,\n",
    "'Colmore Row (Birmingham)': 742,\n",
    "'Ipswich (Corn Exchange)': 743,\n",
    "'Kings Hill': 744,\n",
    "'Chipping Sodbury': 751,\n",
    "'Oakgrove': 752,\n",
    "'Dorking': 755,\n",
    "'Oundle': 758,\n",
    "'Northwich': 759,\n",
    "'Helensburgh': 771,\n",
    "'Monument': 773,\n",
    "'Little Waitrose at John Lewis Watford': 781,\n",
    "'Victoria Street': 783,\n",
    "'Vauxhall': 789,\n",
    "'Horley - Brighton Road': 802,\n",
    "'Wimborne': 805,\n",
    "'Headington - London Road': 806,\n",
    "'Guildford Worplesdon Road': 808,\n",
    "'Little Waitrose John Lewis Southampton': 815,\n",
    "'East Putney': 820,\n",
    "'Meanwood': 828,\n",
    "'Chester': 842,\n",
    "'Raynes Park': 846,\n",
    "'Oadby': 847,\n",
    "'Leatherhead': 859,\n",
    "'Victoria Bressenden Place': 860,\n",
    "'SKY (OSTERLEY)': 865,\n",
    "'Faringdon': 871,\n",
    "'Haywards Heath': 873,\n",
    "'Banbury': 874,\n",
    "'Finchley Central': 876,\n",
    "'Bromsgrove': 877,\n",
    "'Winchmore Hill': 878,\n",
    
    
    "}\n",
    "\n",
    "England = ['London']\n",
    "Wales = ['Clwyd', 'Dyfed', 'Gwent', 'Gwynedd', 'Mid-Glamorgan', 'Powys', 'South-Glamorgan', 'West-Glamorgan']\n",
    "Scotland = ['Aberdeenshire', 'Angus', 'Argyll', 'Ayrshire', 'Banffshire', 'Berwickshire', 'Bute', 'Caithness',\n",
    "            'Clackmannanshire', 'Dumfriesshire', 'Dunbartonshire', 'East-Lothian', 'Fife', 'Inverness-shire',\n",
    "            'Kincardineshire', 'Kinross-shire', 'Kirkcudbrightshire', 'Lanarkshire', 'Midlothian', 'Moray', 'Nairnshire',\n",
    "            'Orkney', 'Peeblesshire', 'Perthshire', 'Renfrewshire', 'Ross-shire', 'Roxburghshire', 'Selkirkshire',\n",
    "            'Shetland', 'Stirlingshire', 'Sutherland', 'West Lothian', 'Wigtownshire']\n",
    "NorthernIreland = ['Antrim', 'Armagh', 'Down', 'Fermanagh', 'Londonderry', 'Tyrone']\n",
    "\n",
    "all_branch_keyword = list(branch_keyword_bu_num.keys())\n",
    "branch_keyword = all_branch_keyword\n",
    "countries = [England]\n",
    "final = []\n",
    "status_val = []\n",
    "road = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"name":"text_helpers"},
   "outputs": [],
   "source": [
    "def _removeNonAscii(s):\n",
    "    return \"\".join(i for i in s if ord(i) < 128)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = text.replace('(ap)', '')\n",
    "    text = re.sub(r\"\\'s\", \" is \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r\"\\\\\", \"\", text)\n",
    "    text = re.sub(r\"\\'\", \"\", text)\n",
    "    text = re.sub(r'\\\"', '', text)\n",
    "    text = re.sub('[^a-zA-Z ?!]+', '', text)\n",
    "    text = _removeNonAscii(text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(word_tokens):\n",
    "    filtered_sentence = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    specific_words_list = ['char', 'u', 'hindustan', 'doj', 'washington']\n",
    "    stop_words.extend(specific_words_list)\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence\n",
    "\n",
    "def lemmatize(x):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "def tokenize(x):\n",
    "    return tokenizer.tokenize(x)\n",
    "\n",
    "nltk.download('all')\n",
    "# (Optional minimal)\n",
    "# for pkg in (\"punkt\", \"stopwords\", \"wordnet\", \"vader_lexicon\"):\n",
    "#     nltk.download(pkg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"name":"rss_collector"},
   "outputs": [],
   "source": [
    "def googleNewsByStreet():\n",
    "    \"\"\"RSS-based collector for the last ~15 days; appends DataFrame to global 'final'.\"\"\"\n",
    "    global final\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    now_utc = datetime.now(timezone.utc)\n",
    "    cutoff = now_utc - timedelta(days=7)\n",
    "    base_rss = \"https://news.google.com/rss/search?q={query}&hl=en-GB&gl=GB&ceid=GB:en&scoring=n\"\n",
    "\n",
    "    for branch in branch_keyword:\n",
    "        for keyword in keywords:\n",
    "            query = f\"{branch} {keyword} when:7d\"\n",
    "            rss_url = base_rss.format(query=quote_plus(query))\n",
    "            try:\n",
    "                feed = feedparser.parse(rss_url)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] RSS parse failed for {query}: {e}\")\n",
    "                continue\n",
    "\n",
    "            rows = []\n",
    "            for entry in getattr(feed, \"entries\", []):\n",
    "                dt_parsed = None\n",
    "                if hasattr(entry, \"published_parsed\") and entry.published_parsed:\n",
    "                    dt_parsed = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)\n",
    "                elif hasattr(entry, \"updated_parsed\") and entry.updated_parsed:\n",
    "                    dt_parsed = datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)\n",
    "                else:\n",
    "                    text_date = entry.get(\"published\") or entry.get(\"updated\") or \"\"\n",
    "                    if text_date:\n",
    "                        try:\n",
    "                            dt_parsed = datetime.fromisoformat(text_date)\n",
    "                            if dt_parsed.tzinfo is None:\n",
    "                                dt_parsed = dt_parsed.replace(tzinfo=timezone.utc)\n",
    "                        except Exception:\n",
    "                            dt_parsed = None\n",
    "\n",
    "                if dt_parsed is not None and dt_parsed < cutoff:\n",
    "                    continue\n",
    "\n",
    "                title = entry.get(\"title\", \"\")\n",
    "                link = entry.get(\"link\", \"\")\n",
    "                published_str = entry.get(\"published\", \"\") or entry.get(\"updated\", \"\")\n",
    "                media = \"\"\n",
    "                try:\n",
    "                    media = entry.source.title\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "                if not title:\n",
    "                    continue\n",
    "\n",
    "                rows.append({\n",
    "                    \"title\": str(title),\n",
    "                    \"media\": str(media),\n",
    "                    \"datetime\": dt_parsed.isoformat() if dt_parsed is not None else str(published_str),\n",
    "                    \"link\": str(link),\n",
    "                    \"keyword\": str(keyword),\n",
    "                    \"branch\": str(branch),\n",
    "                    \"bu_num\": branch_keyword_bu_num.get(branch, None)\n",
    "                })\n",
    "\n",
    "            if rows:\n",
    "                df = pd.DataFrame(rows).drop_duplicates(subset=[\"title\"], keep=\"first\")\n",
    "                if not df.empty:\n",
    "                    data = pd.concat([data, df], ignore_index=True)\n",
    "\n",
    "    if not data.empty:\n",
    "        final.append(data)\n",
    "    else:\n",
    "        print(\"[INFO] Collector produced no rows (check runner network/proxy or queries).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"name":"sentiment"},
   "outputs": [],
   "source": [
    "def sentiment_analysis(prod):\n",
    "    prod['combined_text'] = prod['title'].map(str)\n",
    "    prod['combined_text'] = prod['combined_text'].map(clean_text)\n",
    "    prod['tokens'] = prod['combined_text'].map(tokenize)\n",
    "    prod['tokens'] = prod['tokens'].map(remove_stopwords)\n",
    "    prod['lems'] = prod['tokens'].map(lemmatize)\n",
    "    sia = SIA()\n",
    "    results = []\n",
    "    for line in prod['lems']:\n",
    "        pol_score = sia.polarity_scores(line)\n",
    "        pol_score['lems'] = line\n",
    "        results.append(pol_score)\n",
    "    headlines_polarity = pd.DataFrame.from_records(results)\n",
    "    headlines_polarity['label'] = 0\n",
    "    headlines_polarity.loc[headlines_polarity['compound'] > 0.2, 'label'] = 1\n",
    "    headlines_polarity.loc[headlines_polarity['compound'] < -0.2, 'label'] = -1\n",
    "    headlines_polarity['word_count'] = headlines_polarity['lems'].apply(lambda x: len(str(x).split()))\n",
    "    headlines_polarity = headlines_polarity.rename_axis(index=None)\n",
    "    return pd.merge(prod, headlines_polarity, on=[\"lems\"], how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"name":"outsource_news"},
   "outputs": [],
   "source": [
    "def outsource_news():\n",
    "    googleNewsByStreet()\n",
    "    prod = pd.concat(final) if final else pd.DataFrame()\n",
    "    prod = prod.drop_duplicates('title', keep='first') if not prod.empty else prod\n",
    "    print(prod)\n",
    "    status_val.append(30)\n",
    "    if prod.empty:\n",
    "        return prod\n",
    "    final_prod = sentiment_analysis(prod)\n",
    "    final_prod = final_prod.replace(np.nan,'',regex=True)\n",
    "\n",
    "    store_keywords = ['opens', 'closes', 'closed', 'opened', 'open', 'close','shut', 'confining', 'unopen', 'opening',\n",
    "                      'close down', 'closing', 'shut down', 'conclude', 'ending', 'shutdown', 'closedown','closure', 'temporary',\n",
    "                      'extended', 'shutting', 'launch', 'shuts', 'closures']\n",
    "    competitor_keywords = ['tesco', 'wendys', 'lidl', 'sainsburys', 'sainsbury', 'aldi', 'morrisons', 'spencer', 'asda',\n",
    "                           'supermarket','co', 'ocado', 'sparks', 'b&m', 'iceland', 'waitrose']\n",
    "\n",
    "    print(final_prod)\n",
    "    for index, row in final_prod.iterrows():\n",
    "        toks = row.get('tokens', [])\n",
    "        if len(np.intersect1d(toks, store_keywords)) == 0:\n",
    "            final_prod.drop(index=index, axis=0, inplace=True)\n",
    "        else:\n",
    "            if len(np.intersect1d(toks, competitor_keywords)) == 0:\n",
    "                final_prod.drop(index=index, axis=0, inplace=True)\n",
    "\n",
    "    for index, row in final_prod.iterrows():\n",
    "        for value in row.get('tokens', []):\n",
    "            val = value.capitalize()\n",
    "            if val in branch_keyword_bu_num:\n",
    "                final_prod.at[index,'bu_num'] = branch_keyword_bu_num[val]\n",
    "                final_prod.at[index,'branch'] = val\n",
    "\n",
    "    final_prod = final_prod.drop_duplicates('title', keep='first')\n",
    "    final_prod = final_prod.drop_duplicates('lems', keep='first')\n",
    "    final_prod = final_prod.drop_duplicates('tokens', keep='first')\n",
    "    final_prod['title'] = final_prod['title'].astype(str)\n",
    "    final_prod['competitor_evt_indchar'] = ['Yes' if(len(np.intersect1d(x,competitor_keywords)) > 0) else 'No' for x in final_prod['tokens']]\n",
    "    counter_guid = int(date.today().strftime(\"%Y%m%d\"))\n",
    "    final_prod['efsevt_guid'] = [(counter_guid*1000)+i for i in range(len(final_prod))]\n",
    "\n",
    "    print(final_prod.dtypes)\n",
    "    print(final_prod_events.dtypes)\n",
    "    foriegn_key = []\n",
    "    for index, row in final_prod.iterrows():\n",
    "        flag = False\n",
    "        for index_event, row_event in final_prod_events.iterrows():\n",
    "            if(row.get('keyword','') != '' ):\n",
    "                if(row['keyword'] in row_event.get('NAME','')):\n",
    "                    print(row['keyword'],row_event['NAME'])\n",
    "                    foriegn_key.append(row_event.get('GUID',0))\n",
    "                    flag = True\n",
    "        if(flag == False):\n",
    "            foriegn_key.append(0)\n",
    "    print(foriegn_key)\n",
    "\n",
    "    final_prod['guid'] = [(counter_guid*2000)+i for i in range(len(final_prod))]\n",
    "    final_prod['fixed_annual_ind'] = 'n'\n",
    "    final_prod['perm_env_ind'] = 'n'\n",
    "    final_prod['cancelled_ind'] = 'n'\n",
    "    final_prod['create_user'] = ''\n",
    "    final_prod['update_user'] = ''\n",
    "    final_prod['perm_env_ind'] = 'n'\n",
    "    final_prod['crt_timestamp'] = date.today()\n",
    "    final_prod['upd_timestamp'] = date.today()\n",
    "\n",
    "    final_prod.rename(columns={'link':'source_of_event'}, inplace=True)\n",
    "    final_prod[['datetime']] = final_prod[['datetime']].astype(str)\n",
    "    final_prod.columns = final_prod.columns.str.upper()\n",
    "    final_prod.to_csv('Events.csv', mode='a', index=False, header=False)\n",
    "    return final_prod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"name":"geo_distance_basic"},
   "outputs": [],
   "source": [
    "def distance(lat1, lon1, lat2, lon2):\n",
    "    p = pi/180\n",
    "    a = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p) * cos(lat2*p) * (1-cos((lon2-lon1)*p))/2\n",
    "    return round(12742 * asin(sqrt(a)),2)\n",
    "\n",
    "def place_distance(string1,string2):\n",
    "    try:\n",
    "        geolocator_addr = Nominatim(user_agent=\"efs\")\n",
    "        place = string1 + \",\" + string2\n",
    "        place_2 = \"Waitrose,\" + string2\n",
    "        pin = geolocator_addr.geocode(place)\n",
    "        pin_2 = geolocator_addr.geocode(place_2)\n",
    "        print(pin)\n",
    "        print(pin_2)\n",
    "        print(pin.raw['lat'],pin.raw['lon'],pin_2.raw['lat'],pin_2.raw['lon'])\n",
    "    except:\n",
    "        return 'N/A'\n",
    "    return distance(float(pin.raw['lat']),float(pin.raw['lon']),float(pin_2.raw['lat']),float(pin_2.raw['lon']))\n",
    "\n",
    "road = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"name":"infra_scraper"},
   "outputs": [],
   "source": [
    "def distance_infrastructure(lat1, lon1, lat2, lon2):\n",
    "    p = pi/180\n",
    "    a = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p) * cos(lat2*p) * (1-cos((lon2-lon1)*p))/2\n",
    "    return round(12742 * asin(sqrt(a)),2)\n",
    "\n",
    "def place_distance_infrastructure(string2, work):\n",
    "    temp = []\n",
    "    try:\n",
    "        route_df = pd.read_csv(\"Branch_Lat_Lon.csv\")\n",
    "        geolocator_addr = Nominatim(user_agent=\"http\")\n",
    "        place_2 = string2 + \", London, UK\"\n",
    "        pin_2 = geolocator_addr.geocode(place_2)\n",
    "        print(pin_2)\n",
    "        print(pin_2.raw['lat'],pin_2.raw['lon'])\n",
    "        actual = 999999\n",
    "        if(pin_2 != 'None'):\n",
    "            for index, row in route_df.iterrows():\n",
    "                dif = distance_infrastructure(float(pin_2.raw['lat']),float(pin_2.raw['lon']),row[\"lat\"],row[\"lon\"])\n",
    "                if( dif < 20):\n",
    "                    if(actual > dif):\n",
    "                        actual = dif\n",
    "                        branch_name = row[\"branch\"]\n",
    "        if(actual <= 1):\n",
    "            print(actual)\n",
    "            temp.append(branch_name)\n",
    "            print(branch_name)\n",
    "            print(\"Yes\")\n",
    "            road.append([ string2 + \" \" + work, branch_name, \"\", date.today(), branch_keyword_bu_num.get(branch_name,''), actual,\"\"])\n",
    "            return branch_name\n",
    "        print(actual)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def infrastructure():\n",
    "    url = (\n",
    "        \"https://tfl.gov.uk/traffic/status/?Input=&lineIds=&dateTypeSelect=Future%20date&direction=\"\n",
    "        + \"&startDate=\" + date.today().strftime(\"%Y-%m-%d\") + \"T00%3A00%3A00\"\n",
    "        + \"&endDate=\" + date.today().strftime(\"%Y-%m-%d\") + \"T23%3A59%3A59\"\n",
    "        + \"&lat=51.50721740722656&lng=-0.12758620083332062&placeType=stoppoint&input=London%2C%20UK\"\n",
    "    )\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "    ele = soup.select('div[class^=\"road-disruption\"]')\n",
    "    street, works = [], []\n",
    "    for element in ele:\n",
    "        h2_tags = element.select('h4')\n",
    "        p_tags = element.select('p[class^=\"topmargin\"]')\n",
    "        date_tags = element.select('p[class^=\"highlight dates\"]')\n",
    "        if date_tags:\n",
    "            print(date_tags[0].text.strip(\"\\n\\n\").split(\"\\n\"))\n",
    "        for h2_tag,p_tag in zip(h2_tags,p_tags):\n",
    "            if(\"Works\" in p_tag.text):\n",
    "                arr = h2_tag.text.strip().split(\" \")\n",
    "                word = \"\"\n",
    "                for i in range(1,len(arr)-1):\n",
    "                    if('(' not in arr[i]):\n",
    "                        word = word + \" \" + arr[i]\n",
    "                street.append(word)\n",
    "                works.append(p_tag.text)\n",
    "    street = list(set(street))\n",
    "    print(len(street))\n",
    "    for i,j in zip(street,works):\n",
    "        place_distance_infrastructure(i,j)\n",
    "    return road\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"name":"email_sender"},
   "outputs": [],
   "source": [
    "def mail_data(final_prod, final_prod_old):\n",
    "    port = 465  # SSL\n",
    "    context = ssl.create_default_context()\n",
    "    mail_df = pd.DataFrame()\n",
    "\n",
    "    if final_prod.empty:\n",
    "        mail_df = pd.DataFrame([{\n",
    "            \"TITLE\": \"No items captured\",\n",
    "            \"BRANCH\": \"\",\n",
    "            \"SOURCE\": \"\",\n",
    "            \"DATETIME\": date.today().strftime(\"%Y-%m-%d\"),\n",
    "            \"BRANCH_NUM\": \"\",\n",
    "            \"DISTANCE IN MILES\": \"\",\n",
    "            \"LINK\": \"\"\n",
    "        }])\n",
    "    else:\n",
    "        mail_df[\"TITLE\"] = final_prod[\"TITLE\"]\n",
    "        mail_df[\"BRANCH\"] = final_prod[\"BRANCH\"]\n",
    "        mail_df[\"SOURCE\"] = final_prod[\"MEDIA\"]\n",
    "        mail_df[\"DATETIME\"] = final_prod[\"DATETIME\"]\n",
    "        mail_df[\"BRANCH_NUM\"] = final_prod[\"BU_NUM\"]\n",
    "        distance_arr, urls = [], []\n",
    "        for _, row in final_prod.iterrows():\n",
    "            if(row.get(\"KEYWORD\") and row.get(\"BRANCH\")):\n",
    "                distance_arr.append(place_distance(row[\"KEYWORD\"], row[\"BRANCH\"]))\n",
    "            try:\n",
    "                urls.append(Shortener().tinyurl.short(row.get(\"SOURCE_OF_EVENT\",\"\")))\n",
    "            except Exception:\n",
    "                urls.append(row.get(\"SOURCE_OF_EVENT\",\"\"))\n",
    "        if distance_arr:\n",
    "            mail_df[\"DISTANCE IN MILES\"] = distance_arr\n",
    "        mail_df[\"LINK\"] = urls if urls else \"\"\n",
    "        for index, row in mail_df.iterrows():\n",
    "            if(row.get(\"DISTANCE IN MILES\") != 'N/A'):\n",
    "                if(isinstance(row.get(\"DISTANCE IN MILES\"), (int, float)) and row[\"DISTANCE IN MILES\"] > 25):\n",
    "                    mail_df.drop(index=index, axis=0, inplace=True)\n",
    "\n",
    "    all_prod = pd.concat([mail_df, final_prod_old]) if not mail_df.empty else mail_df\n",
    "\n",
    "    # Make road scraping non-fatal\n",
    "    road_list = []\n",
    "    try:\n",
    "        road_list = infrastructure()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] infrastructure() failed: {e}\")\n",
    "        road_list = []\n",
    "\n",
    "    road_df = (pd.DataFrame(\n",
    "        road_list,\n",
    "        columns=[\"TITLE\",\"BRANCH\",\"SOURCE\",\"DATETIME\",\"BRANCH_NUM\",\"DISTANCE IN MILES\",\"LINK\"]\n",
    "    ) if road_list else pd.DataFrame(\n",
    "        columns=[\"TITLE\",\"BRANCH\",\"SOURCE\",\"DATETIME\",\"BRANCH_NUM\",\"DISTANCE IN MILES\",\"LINK\"]\n",
    "    ))\n",
    "    road_df = road_df.drop_duplicates(\"BRANCH\", keep=\"first\") if not road_df.empty else road_df\n",
    "\n",
    "    html_table = mail_df.to_html(index=False, classes='example-table')\n",
    "    road_table = road_df.to_html(index=False, classes='example-table')\n",
    "\n",
    "    text = (\n",
    "        f\"Hello Alex and Tim,\\n Herewith attaching the events captured for all the competitors (core event types) \\n\"\n",
    "        f\"including all the branches from {(date.today() - timedelta(days = 1)).strftime('%d-%m-%Y')} to {date.today().strftime('%d-%m-%Y')} \\n\"\n",
    "        f\"which are auto-generated from the script.\\n\\n\\nThanks And Regards,\\nSubhash\\n\\n\\n\"\n",
    "    )\n",
    "\n",
    "    html_table = html_table.replace('<th>', '<th style=\"padding: 10px 90px 10px 90px;\">', 1)\n",
    "    road_table = road_table.replace('<th>', '<th style=\"padding: 10px 80px 10px 80px;\">', 1)\n",
    "\n",
    "    html = f'''\n",
    "<html>\n",
    "<head>\n",
    " <style>\n",
    " table.example-table th{{ padding: 10px; text-align: center; background-color: #FFFFFF; font-weight: bold; font-size: 14px; width: 400px; }}\n",
    " table.example-table th:first-child {{ padding: 20px 100px 20px 100px; }}\n",
    " table.example-table td {{ padding: 5px; color: black; font-size: 12px; width: 400px; font-family: Century Gothic, sans-serif; }}\n",
    " </style>\n",
    "</head>\n",
    "<body>\n",
    " <pre>{text}</pre>\n",
    " {html_table}\n",
    " <br/><br/>\n",
    " {road_table}\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "    part1 = MIMEText(html, 'html')\n",
    "    msg = MIMEMultipart(\"alternative\")\n",
    "    msg['Subject'] = \"Automated Event Capturing Model\"\n",
    "    recipients = ['subhash.verma@johnlewis.co.uk','amit.kumbhar@johnlewis.co.uk', 'abhishek.jambhale@johnlewis.co.uk']\n",
    "    msg['To'] = \", \".join(recipients)\n",
    "    msg.attach(part1)\n",
    "\n",
    "    file_name = date.today().strftime(\"%d-%m-%Y\") + \"_Events.xlsx\"\n",
    "    textStream = BytesIO()\n",
    "    writer = pd.ExcelWriter(textStream, engine='xlsxwriter')\n",
    "    all_prod.to_excel(writer, sheet_name=\"Competitor Events\", index=False)\n",
    "    road_df.to_excel(writer, sheet_name=\"Road Closure Events\", index=False)\n",
    "    writer.close()\n",
    "    textStream.seek(0)\n",
    "    attachment = MIMEApplication(textStream.read(), name=file_name)\n",
    "    attachment['Content-Disposition'] = 'attachment; filename=\"{}\"'.format(file_name)\n",
    "    msg.attach(attachment)\n",
    "\n",
    "    with smtplib.SMTP_SSL(\"smtp.gmail.com\", port, context=context) as server:\n",
    "        dec = str(Fernet('egupkHT3QJHG1c5dcPGiWEZaWdH04_uhgyD-8lYNxWM=').decrypt(\n",
    "            b'gAAAAABpHy2IRPVaNZJU3a2jDD68rGtj0jMYEvJyrWRJepy-wUXuHwKdmAzMTSDXAWkP4S8tUWCd6Q5egqHWKGFkMx18sIu6NUPerPx9TSkeFpCedLP3LAc=',\n",
    "        ), 'UTF-8')\n",
    "        print(\"IN\")\n",
    "        server.login(\"subhash.verma@johnlewis.co.uk\", dec)\n",
    "        server.sendmail(\"subhash.verma@johnlewis.co.uk\", recipients, msg.as_string())\n",
    "    return all_prod, road_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"name":"main"},
   "outputs": [],
   "source": [
    "import gspread\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    final_prod = outsource_news()\n",
    "    final_prod_old = pd.DataFrame()\n",
    "    all_prod, road_df = mail_data(final_prod, final_prod_old)\n",
    "    print(\"mail sent\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}


















