
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install feedparser\n",
    "!pip install GoogleNews\n",
    "!pip install gnewsclient\n",
    "!pip install snscrape==0.6.2.20230320\n",
    "!pip install rake_nltk\n",
    "!pip install geopy\n",
    "!pip install pgeocode\n",
    "!pip install requests lxml\n",
    "!pip install beautifulsoup4\n",
    "!pip install pretty-html-table\n",
    "!pip install pyshorteners\n",
    "!pip install xlsxwriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from GoogleNews import GoogleNews\n",
    "from gnewsclient import gnewsclient\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "from datetime import datetime, date, timedelta\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from rake_nltk import Rake\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "# NEW: RSS-based fetching & helpers\n",
    "import feedparser\n",
    "from urllib.parse import quote_plus\n",
    "import time\n",
    "from datetime import datetime as dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def http_get_with_retries(session, url, headers=None, tries=3, backoff=2.0, timeout=30):\n",
    "    \"\"\"GET with simple exponential backoff; returns Response or raises last error.\"\"\"\n",
    "    last_exc = None\n",
    "    for i in range(tries):\n",
    "        try:\n",
    "            r = session.get(url, headers=headers or {}, timeout=timeout, allow_redirects=True)\n",
    "            if r.status_code == 200 and r.content:\n",
    "                return r\n",
    "            else:\n",
    "                print(f\"[HTTP] status={r.status_code} len={len(r.content)} url={url}\")\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "            print(f\"[HTTP-ERR] try={i+1}/{tries} url={url} err={e}\")\n",
    "        time.sleep(backoff ** i)\n",
    "    if last_exc:\n",
    "        raise last_exc\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = []\n",
    "end_date = []\n",
    "sources = [\"bbc-news\", \"the-telegraph\", \"the-guardian-uk\", \"cnn\", \"abc-news-au\",\n",
    "           \"dailymail.co.uk\", \"metro.co.uk\", \"mirror.co.uk\", \"news.google.com\"]\n",
    "all_keywords = ['strike', 'holiday', 'lockdown', 'inflation', 'grocery sales', 'carnival', 'festival', 'party',\n",
    "                'Walmart', 'Tesco', \"Sainsbury's\", 'supply chain', 'flood', 'wendys', 'lidl']\n",
    "keywords = ['Lidl','Waitrose','Tesco','Walmart','Sainsbury\\'s','Aldi','Asda','Marks & Spencers','Morrison\\'s']\n",
    "events = ['autumn bank holiday']\n",
    "all_events = ['autumn bank holiday']\n",
    "final_prod_events = pd.DataFrame()\n",
    "counter = 6000\n",
    "gnews_client_topics = ['Top Stories','World','Nation','Business','Technology','Entertainment','Sports','Science','Health']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "BRANCH_DICT_COMPACT"
    ]
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# COMPACT BRANCH DICTIONARY (replace with yours)\n",
    "# ---------------------------------------------\n",
    "branch_keyword_bu_num = {\n",
    "    'Brighton': 114,\n",
    "    'Dorchester': 120,\n",
    "    'Esher': 121,\n",
   
    "}\n",
    "all_branch_keyword = list(branch_keyword_bu_num.keys())\n",
    "branch_keyword = all_branch_keyword\n",
    "countries = [['London']]\n",
    "final = []\n",
    "status_val = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Text processing helpers ----------\n",
    "def _removeNonAscii(s):\n",
    "    return \"\".join(i for i in s if ord(i) < 128)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = text.replace('(ap)', '')\n",
    "    text = re.sub(r\"\\'s\", \" is \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r'\\\\W+', ' ', text)\n",
    "    text = re.sub(r'\\\\s+', ' ', text)\n",
    "    text = re.sub(r\"\\\\\\\\\", \"\", text)\n",
    "    text = re.sub(r\"\\'\", \"\", text)\n",
    "    text = re.sub(r'\\\"', '', text)\n",
    "    text = re.sub('[^a-zA-Z ?!]+', '', text)\n",
    "    text = _removeNonAscii(text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(word_tokens):\n",
    "    filtered_sentence = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    specific_words_list = ['char', 'u', 'hindustan', 'doj', 'washington']\n",
    "    stop_words.extend(specific_words_list)\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence\n",
    "\n",
    "def lemmatize(x):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\\\w+')\n",
    "\n",
    "def tokenize(x):\n",
    "    return tokenizer.tokenize(x)\n",
    "\n",
    "nltk.download('all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Sentiment ----------\n",
    "def sentiment_analysis(prod):\n",
    "    if prod.empty:\n",
    "        return prod.copy()\n",
    "    prod['combined_text'] = prod['title'].map(str)\n",
    "    prod['combined_text'] = prod['combined_text'].map(clean_text)\n",
    "    prod['tokens'] = prod['combined_text'].map(tokenize)\n",
    "    prod['tokens'] = prod['tokens'].map(remove_stopwords)\n",
    "    prod['lems'] = prod['tokens'].map(lemmatize)\n",
    "    sia = SIA()\n",
    "    results = []\n",
    "    for line in prod['lems']:\n",
    "        pol_score = sia.polarity_scores(line)\n",
    "        pol_score['lems'] = line\n",
    "        results.append(pol_score)\n",
    "    headlines_polarity = pd.DataFrame.from_records(results)\n",
    "    headlines_polarity['label'] = 0\n",
    "    headlines_polarity.loc[headlines_polarity['compound'] > 0.2, 'label'] = 1\n",
    "    headlines_polarity.loc[headlines_polarity['compound'] < -0.2, 'label'] = -1\n",
    "    headlines_polarity['word_count'] = headlines_polarity['lems'].apply(lambda x: len(str(x).split()))\n",
    "    headlines_polarity = headlines_polarity.rename_axis(index=None)\n",
    "    return pd.merge(prod, headlines_polarity, on=[\"lems\"], how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- RSS-based fetch with fallbacks and logging ----------\n",
    "def googleNewsByStreet():\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        \"User-Agent\": (\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "                       \"(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"),\n",
    "        \"Accept\": \"application/rss+xml,text/xml;q=0.9,*/*;q=0.8\",\n",
    "    }\n",
    "\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    def resolve_google_link(url: str) -> str:\n",
    "        try:\n",
    "            if \"news.google.com\" in url:\n",
    "                r = http_get_with_retries(session, url, headers=headers, tries=2, backoff=2.0, timeout=20)\n",
    "                return r.url if r is not None else url\n",
    "        except Exception:\n",
    "            pass\n",
    "        return url\n",
    "\n",
    "    total_pairs = 0\n",
    "    nonempty_pairs = 0\n",
    "\n",
    "    for branch in branch_keyword:\n",
    "        for keyword in keywords:\n",
    "            total_pairs += 1\n",
    "            query = f\"{branch} {keyword}\"\n",
    "            rss_24h = (\n",
    "                f\"https://news.google.com/rss/search?q={quote_plus(query)}+when:1d&hl=en-GB&gl=GB&ceid=GB:en\"\n",
    "            )\n",
    "            rows = []\n",
    "            try:\n",
    "                resp = http_get_with_retries(session, rss_24h, headers=headers, tries=3, backoff=2.0, timeout=30)\n",
    "                feed = feedparser.parse(resp.content if resp is not None else b\"\")\n",
    "                for entry in feed.entries:\n",
    "                    rows.append({\n",
    "                        \"title\": entry.get(\"title\", \"\"),\n",
    "                        \"link\": resolve_google_link(entry.get(\"link\", \"\")),\n",
    "                        \"desc\": entry.get(\"summary\", \"\"),\n",
    "                        \"datetime\": entry.get(\"published\", entry.get(\"updated\", \"\")),\n",
    "                        \"media\": getattr(getattr(entry, \"source\", None), \"title\", \"\")\n",
    "                                 or (entry.get(\"source\", {}) or {}).get(\"title\", \"\"),\n",
    "                        \"keyword\": keyword,\n",
    "                        \"branch\": branch,\n",
    "                        \"bu_num\": branch_keyword_bu_num.get(branch, None)\n",
    "                    })\n",
    "                if not rows:\n",
    "                    rss_3d = (\n",
    "                        f\"https://news.google.com/rss/search?q={quote_plus(query)}+when:3d&hl=en-GB&gl=GB&ceid=GB:en\"\n",
    "                    )\n",
    "                    print(f\"[FALLBACK] 0 items for 24h; trying 3d for '{query}'\")\n",
    "                    resp2 = http_get_with_retries(session, rss_3d, headers=headers, tries=2, backoff=2.0, timeout=30)\n",
    "                    feed2 = feedparser.parse(resp2.content if resp2 is not None else b\"\")\n",
    "                    for entry in feed2.entries:\n",
    "                        rows.append({\n",
    "                            \"title\": entry.get(\"title\", \"\"),\n",
    "                            \"link\": resolve_google_link(entry.get(\"link\", \"\")),\n",
    "                            \"desc\": entry.get(\"summary\", \"\"),\n",
    "                            \"datetime\": entry.get(\"published\", entry.get(\"updated\", \"\")),\n",
    "                            \"media\": getattr(getattr(entry, \"source\", None), \"title\", \"\")\n",
    "                                     or (entry.get(\"source\", {}) or {}).get(\"title\", \"\"),\n",
    "                            \"keyword\": keyword,\n",
    "                            \"branch\": branch,\n",
    "                            \"bu_num\": branch_keyword_bu_num.get(branch, None)\n",
    "                        })\n",
    "                if not rows:\n",
    "                    konly = (\n",
    "                        f\"https://news.google.com/rss/search?q={quote_plus(keyword)}+when:1d&hl=en-GB&gl=GB&ceid=GB:en\"\n",
    "                    )\n",
    "                    print(f\"[FALLBACK] still 0; keyword-only 1d for '{keyword}'\")\n",
    "                    resp3 = http_get_with_retries(session, konly, headers=headers, tries=2, backoff=2.0, timeout=30)\n",
    "                    feed3 = feedparser.parse(resp3.content if resp3 is not None else b\"\")\n",
    "                    for entry in feed3.entries:\n",
    "                        rows.append({\n",
    "                            \"title\": entry.get(\"title\", \"\"),\n",
    "                            \"link\": resolve_google_link(entry.get(\"link\", \"\")),\n",
    "                            \"desc\": entry.get(\"summary\", \"\"),\n",
    "                            \"datetime\": entry.get(\"published\", entry.get(\"updated\", \"\")),\n",
    "                            \"media\": getattr(getattr(entry, \"source\", None), \"title\", \"\")\n",
    "                                     or (entry.get(\"source\", {}) or {}).get(\"title\", \"\"),\n",
    "                            \"keyword\": keyword,\n",
    "                            \"branch\": \"\",\n",
    "                            \"bu_num\": None\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] RSS fetch failed for '{query}': {e}\")\n",
    "            if rows:\n",
    "                nonempty_pairs += 1\n",
    "                df = pd.DataFrame(rows)\n",
    "                data = pd.concat([data, df], ignore_index=True)\n",
    "    data = data.drop(columns=[\"img\", \"site\"], errors=\"ignore\")\n",
    "    print(f\"[STATS] pairs={total_pairs} nonempty={nonempty_pairs} rows={len(data)} at {dt.utcnow().isoformat()}Z\")\n",
    "    try:\n",
    "        data.to_csv(\"rss_debug_snapshot.csv\", index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    final.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outsource_news():\n",
    "    googleNewsByStreet()\n",
    "    prod = pd.concat(final) if len(final) > 0 else pd.DataFrame()\n",
    "    if prod.empty:\n",
    "        print(\"[INFO] No RSS rows captured; continuing for email guard.\")\n",
    "        prod = pd.DataFrame(columns=[\"title\",\"link\",\"desc\",\"datetime\",\"media\",\"keyword\",\"branch\",\"bu_num\"]) \n",
    "    else:\n",
    "        prod = prod.drop_duplicates('title', keep='first')\n",
    "    print(prod)\n",
    "    status_val.append(30)\n",
    "    final_prod = sentiment_analysis(prod) if not prod.empty else prod.copy()\n",
    "    final_prod = final_prod.replace(np.nan,'',regex=True)\n",
    "\n",
    "    # Keyword sets (unchanged from your pipeline)\n",
    "    store_keywords = ['opens','closes','closed','opened','open','close','shut','confining','unopen','opening',\n",
    "                      'close down','closing','shut down','conclude','ending','shutdown','closedown','closure','temporary',\n",
    "                      'extended','shutting','launch','shuts','closures']\n",
    "    competitor_keywords = ['tesco','wendys','lidl','sainsburys','sainsbury','aldi','morrisons','spencer','asda',\n",
    "                           'supermarket','co','ocado','sparks','b&m','iceland','waitrose']\n",
    "\n",
    "    print(final_prod)\n",
    "    if not final_prod.empty:\n",
    "        for index, row in final_prod.iterrows():\n",
    "            toks = row.get('tokens', [])\n",
    "            if len(np.intersect1d(toks, store_keywords)) == 0:\n",
    "                final_prod.drop(index=index, axis=0, inplace=True)\n",
    "            else:\n",
    "                if len(np.intersect1d(toks, competitor_keywords)) == 0:\n",
    "                    final_prod.drop(index=index, axis=0, inplace=True)\n",
    "        for index, row in final_prod.iterrows():\n",
    "            for value in row.get('tokens', []):\n",
    "                val = value.capitalize()\n",
    "                if val in branch_keyword_bu_num:\n",
    "                    final_prod.at[index,'bu_num'] = branch_keyword_bu_num[val]\n",
    "                    final_prod.at[index,'branch'] = val\n",
    "        final_prod = final_prod.drop_duplicates('title', keep='first')\n",
    "        final_prod = final_prod.drop_duplicates('lems', keep='first')\n",
    "        final_prod = final_prod.drop_duplicates('tokens', keep='first')\n",
    "        final_prod['title'] = final_prod['title'].astype(str)\n",
    "        final_prod['competitor_evt_indchar'] = ['Yes' if(len(np.intersect1d(x,competitor_keywords)) > 0) else 'No' for x in final_prod['tokens']]\n",
    "        counter_guid = int(date.today().strftime(\"%Y%m%d\"))\n",
    "        final_prod['efsevt_guid'] = [(counter_guid*1000)+i for i in range(len(final_prod))]\n",
    "        final_prod['guid'] = [(counter_guid*2000)+i for i in range(len(final_prod))]\n",
    "        final_prod['fixed_annual_ind'] = 'n'\n",
    "        final_prod['perm_env_ind'] = 'n'\n",
    "        final_prod['cancelled_ind'] = 'n'\n",
    "        final_prod['create_user'] = ''\n",
    "        final_prod['update_user'] = ''\n",
    "        final_prod['perm_env_ind'] = 'n'\n",
    "        final_prod['crt_timestamp'] = date.today()\n",
    "        final_prod['upd_timestamp'] = date.today()\n",
    "        final_prod.rename(columns={'link':'source_of_event'}, inplace=True)\n",
    "        final_prod[[\"datetime\"]] = final_prod[[\"datetime\"]].astype(str)\n",
    "        final_prod.columns = final_prod.columns.str.upper()\n",
    "        final_prod.to_csv('Events.csv', mode='a', index=False, header=False)\n",
    "    else:\n",
    "        final_prod = pd.DataFrame(columns=[\"TITLE\",\"LINK\",\"DESC\",\"DATETIME\",\"MEDIA\",\"KEYWORD\",\"BRANCH\",\"BU_NUM\"]).copy()\n",
    "    return final_prod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Photon, GoogleV3, Nominatim\n",
    "import pgeocode\n",
    "from math import cos, asin, sqrt, pi\n",
    "\n",
    "def distance(lat1, lon1, lat2, lon2):\n",
    "    p = pi/180\n",
    "    a = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p) * cos(lat2*p) * (1-cos((lon2-lon1)*p))/2\n",
    "    return round(12742 * asin(sqrt(a)),2)\n",
    "\n",
    "def place_distance(string1,string2):\n",
    "    try:\n",
    "        geolocator_addr = Nominatim(user_agent=\"efs\")\n",
    "        place = string1 + \",\" + string2\n",
    "        place_2 = \"Waitrose,\" + string2\n",
    "        pin = geolocator_addr.geocode(place)\n",
    "        pin_2 = geolocator_addr.geocode(place_2)\n",
    "        print(pin)\n",
    "        print(pin_2)\n",
    "        print(pin.raw['lat'],pin.raw['lon'],pin_2.raw['lat'],pin_2.raw['lon'])\n",
    "    except:\n",
    "        return 'N/A'\n",
    "    return distance(float(pin.raw['lat']),float(pin.raw['lon']),float(pin_2.raw['lat']),float(pin_2.raw['lon']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road = []\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "\n",
    "def distance_infrastructure(lat1, lon1, lat2, lon2):\n",
    "    p = pi/180\n",
    "    a = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p) * cos(lat2*p) * (1-cos((lon2-lon1)*p))/2\n",
    "    return round(12742 * asin(sqrt(a)),2)\n",
    "\n",
    "def place_distance_infrastructure(string2, work):\n",
    "    temp = []\n",
    "    try:\n",
    "        route_df = pd.read_csv(\"Branch_Lat_Lon.csv\")\n",
    "        geolocator_addr = Nominatim(user_agent=\"http\")\n",
    "        place_2 = string2 + \", London, UK\"\n",
    "        pin_2 = geolocator_addr.geocode(place_2)\n",
    "        print(pin_2)\n",
    "        print(pin_2.raw['lat'],pin_2.raw['lon'])\n",
    "        actual = 999999\n",
    "        if(pin_2 != 'None'):\n",
    "            for index, row in route_df.iterrows():\n",
    "                dif = distance_infrastructure(float(pin_2.raw['lat']),float(pin_2.raw['lon']),row[\"lat\"],row[\"lon\"])\n",
    "                if( dif < 20):\n",
    "                    if(actual > dif):\n",
    "                        actual = dif\n",
    "                        branch_name = row[\"branch\"]\n",
    "            if(actual <= 1):\n",
    "                print(actual)\n",
    "                temp.append(branch_name)\n",
    "                print(branch_name)\n",
    "                print(\"Yes\")\n",
    "                road.append([ string2 + \" \" + work, branch_name, \"\", date.today(), branch_keyword_bu_num.get(branch_name,''), actual,\"\"])\n",
    "                return branch_name\n",
    "        print(actual)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def infrastructure():\n",
    "    url = \"https://tfl.gov.uk/traffic/status/?Input=&lineIds=&dateTypeSelect=Future%20date&direction=&startDate=\"+date.today().strftime(\"%Y-%m-%d\")+\"T00%3A00%3A00&endDate=\"+date.today().strftime(\"%Y-%m-%d\")+\"T23%3A59%3A59&lat=51.50721740722656&lng=-0.12758620083332062&placeType=stoppoint&input=London%2C%20UK\"\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "    ele = soup.select('div[class^=\"road-disruption\"]')\n",
    "    street = []\n",
    "    works = []\n",
    "    for element in ele:\n",
    "        h2_tags = element.select('h4')\n",
    "        p_tags = element.select('p[class^=\"topmargin\"]')\n",
    "        date_tags = element.select('p[class^=\"highlight dates\"]')\n",
    "        if date_tags:\n",
    "            print(date_tags[0].text.strip(\"\\n\\n\").split(\"\\n\"))\n",
    "        for h2_tag,p_tag in zip(h2_tags,p_tags):\n",
    "            if(\"Works\" in p_tag.text):\n",
    "                arr = h2_tag.text.strip().split(\" \")\n",
    "                word = \"\"\n",
    "                for i in range(1,len(arr)-1):\n",
    "                    if('(' not in arr[i]):\n",
    "                        word = word + \" \" + arr[i]\n",
    "                street.append(word)\n",
    "                works.append(p_tag.text)\n",
    "    street = list(set(street))\n",
    "    print(len(street))\n",
    "    for i,j in zip(street,works):\n",
    "        place_distance_infrastructure(i,j)\n",
    "    return road\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib, ssl\n",
    "from smtplib import SMTP\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from pretty_html_table import build_table\n",
    "from pyshorteners import Shortener\n",
    "from io import BytesIO\n",
    "from email.mime.application import MIMEApplication\n",
    "from cryptography.fernet import Fernet\n",
    "\n",
    "def mail_data(final_prod, final_prod_old):\n",
    "    port = 465 # For SSL\n",
    "    context = ssl.create_default_context()\n",
    "    mail_df = pd.DataFrame()\n",
    "    if final_prod.empty:\n",
    "        mail_df = pd.DataFrame([{\n",
    "            \"TITLE\": \"No items captured\",\n",
    "            \"BRANCH\": \"\",\n",
    "            \"SOURCE\": \"\",\n",
    "            \"DATETIME\": date.today().strftime(\"%Y-%m-%d\"),\n",
    "            \"BRANCH_NUM\": \"\",\n",
    "            \"DISTANCE IN MILES\": \"\",\n",
    "            \"LINK\": \"\"\n",
    "        }])\n",
    "    else:\n",
    "        mail_df[\"TITLE\"] = final_prod[\"TITLE\"]\n",
    "        mail_df[\"BRANCH\"] = final_prod[\"BRANCH\"]\n",
    "        mail_df[\"SOURCE\"] = final_prod[\"MEDIA\"]\n",
    "        mail_df[\"DATETIME\"] = final_prod[\"DATETIME\"]\n",
    "        mail_df[\"BRANCH_NUM\"] = final_prod[\"BU_NUM\"]\n",
    "        distance_arr = []\n",
    "        for index, row in final_prod.iterrows():\n",
    "            if(row.get(\"KEYWORD\") and row.get(\"BRANCH\")):\n",
    "                distance_arr.append(place_distance(row[\"KEYWORD\"], row[\"BRANCH\"]))\n",
    "        if distance_arr:\n",
    "            mail_df[\"DISTANCE IN MILES\"] = distance_arr\n",
    "        urls = []\n",
    "        for index,row in final_prod.iterrows():\n",
    "            try:\n",
    "                x = Shortener().tinyurl.short(row.get(\"SOURCE_OF_EVENT\",\"\"))\n",
    "            except Exception:\n",
    "                x = row.get(\"SOURCE_OF_EVENT\",\"\")\n",
    "            urls.append(x)\n",
    "        mail_df[\"LINK\"] = urls if urls else \"\"\n",
    "        for index, row in mail_df.iterrows():\n",
    "            if(row.get(\"DISTANCE IN MILES\") != 'N/A'):\n",
    "                if(isinstance(row.get(\"DISTANCE IN MILES\"), (int, float)) and row[\"DISTANCE IN MILES\"] > 25):\n",
    "                    mail_df.drop(index=index, axis=0, inplace=True)\n",
    "    all_prod = pd.concat([mail_df, final_prod_old]) if not mail_df.empty else mail_df\n",
    "    road_list = infrastructure()\n",
    "    print(road_list)\n",
    "    road_df = pd.DataFrame(road_list, columns = [\"TITLE\", \"BRANCH\", \"SOURCE\", \"DATETIME\", \"BRANCH_NUM\", \"DISTANCE IN MILES\", \"LINK\"]) if road_list else pd.DataFrame(columns=[\"TITLE\",\"BRANCH\",\"SOURCE\",\"DATETIME\",\"BRANCH_NUM\",\"DISTANCE IN MILES\",\"LINK\"]) \n",
    "    road_df = road_df.drop_duplicates(\"BRANCH\", keep=\"first\") if not road_df.empty else road_df\n",
    "    html_table = mail_df.to_html(index=False, classes='example-table')\n",
    "    road_table = road_df.to_html(index=False, classes='example-table')\n",
    "    text = f'''Hello Alex and Tim,\\n Herewith attaching the events captured for all the competitors (core event types) including all the branches from {(date.today() - timedelta(days = 1)).strftime('%d-%m-%Y')} to {date.today().strftime('%d-%m-%Y')} which are auto-generated from the script.\\n\\n\\nThanks And Regards,\\nSubhash\\n\\n\\n\\n'''\n",
    "    html_table = html_table.replace('<th>', '<th style=\"padding: 10px 90px 10px 90px;\">', 1)\n",
    "    road_table = road_table.replace('<th>', '<th style=\"padding: 10px 80px 10px 80px;\">', 1)\n",
    "    html = f'''\n",
    "<html>\n",
    "<head>\n",
    " <style>\n",
    " table.example-table th{{ padding: 10px; text-align: center; background-color: #FFFFFF; font-weight: bold; font-size: 14px; width: 400px; }}\n",
    " table.example-table th:first-child {{ padding: 20px 100px 20px 100px; }}\n",
    " table.example-table td {{ padding: 5px; color: black; font-size: 12px; width: 400px; font-family: Century Gothic, sans-serif; }}\n",
    " </style>\n",
    "</head>\n",
    "<body>\n",
    " <pre>{text}</pre>\n",
    " {html_table}\n",
    " <br/><br/>\n",
    " {road_table}\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "    part1 = MIMEText(html, 'html')\n",
    "    msg = MIMEMultipart(\"alternative\")\n",
    "    msg['Subject'] = \"Automated Event Capturing Model\"\n",
    "    recipients = ['subhash.verma@johnlewis.co.uk']\n",
    "    msg['To'] = \", \".join(recipients)\n",
    "    msg.attach(part1)\n",
    "    file_name = date.today().strftime(\"%d-%m-%Y\") + \"_Events.xlsx\"\n",
    "    textStream = BytesIO()\n",
    "    writer = pd.ExcelWriter(textStream, engine='xlsxwriter')\n",
    "    all_prod.to_excel(writer,sheet_name=\"Competitor Events\",index=False)\n",
    "    road_df.to_excel(writer,sheet_name=\"Road Closure Events\",index=False)\n",
    "    writer.close()\n",
    "    textStream.seek(0)\n",
    "    attachment = MIMEApplication(textStream.read(), name= file_name)\n",
    "    attachment['Content-Disposition'] = 'attachment; filename=\"{}\"'.format(file_name)\n",
    "    msg.attach(attachment)\n",
    "    with smtplib.SMTP_SSL(\"smtp.gmail.com\", port, context=context) as server:\n",
    "        dec = str(Fernet('egupkHT3QJHG1c5dcPGiWEZaWdH04_uhgyD-8lYNxWM=').decrypt(b'gAAAAABpHy2IRPVaNZJU3a2jDD68rGtj0jMYEvJyrWRJepy-wUXuHwKdmAzMTSDXAWkP4S8tUWCd6Q5egqHWKGFkMx18sIu6NUPerPx9TSkeFpCedLP3LAc='), 'UTF-8')\n",
    "        print(\"IN\")\n",
    "        server.login(\"subhash.verma@johnlewis.co.uk\", dec)\n",
    "        server.sendmail(\"subhash.verma@johnlewis.co.uk\", recipients, msg.as_string())\n",
    "    return all_prod, road_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "if __name__ == '__main__':\n",
    "    final_prod = outsource_news()\n",
    "    final_prod_old = pd.DataFrame()\n",
    "    all_prod, road_df = mail_data(final_prod, final_prod_old)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

