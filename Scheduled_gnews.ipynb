
# efs_events.py
# Python 3.10+ recommended
import os
import re
import math
from math import cos, asin, sqrt, pi
from datetime import datetime, date, timedelta
import json
import ssl
import smtplib
from smtplib import SMTP
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.application import MIMEApplication
from io import BytesIO

import numpy as np
import pandas as pd
import requests
from bs4 import BeautifulSoup

# News & NLP
from GoogleNews import GoogleNews
from rake_nltk import Rake
import nltk
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA

# URL shortener
from pyshorteners import Shortener

# Geocoding
from geopy.geocoders import Nominatim

# Excel
import xlsxwriter

# ---------------------------
# Setup: ensure NLTK data
# ---------------------------

def ensure_nltk_data():
    """
    Download only the required NLTK resources once.
    """
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt')
    try:
        nltk.data.find('corpora/stopwords')
    except LookupError:
        nltk.download('stopwords')
    try:
        nltk.data.find('corpora/wordnet')
    except LookupError:
        nltk.download('wordnet')
    try:
        nltk.data.find('sentiment/vader_lexicon')
    except LookupError:
        nltk.download('vader_lexicon')


# ---------------------------
# Globals (matching your notebook)
# ---------------------------

start_date = []
end_date = []

sources = ["bbc-news", "the-telegraph", "the-guardian-uk", "cnn", "abc-news-au",
           "dailymail.co.uk", "metro.co.uk", "mirror.co.uk", "news.google.com"]

all_keywords = ['strike', 'holiday', 'lockdown',
                'inflation', 'grocery sales', 'carnival', 'festival', 'party', 'Walmart',
                "Tesco", "Sainsbury's", "supply chain", "flood", "wendys", "lidl"]

keywords = ['Lidl', 'Waitrose', 'Tesco', 'Walmart', "Sainsbury's", 'Aldi', 'Asda', 'Marks & Spencers', "Morrison's"]

events = ['autumn bank holiday']
all_events = ['autumn bank holiday']

final_prod_events = pd.DataFrame()

counter = 6000

gnews_client_topics = ['Top Stories', 'World', 'Nation', 'Business', 'Technology',
                       'Entertainment', 'Sports', 'Science', 'Health']

# Huge branch mapping (unchanged, trimmed only for length in this snippet).
# Keep your full dictionary content here exactly as in your file.
branch_keyword_bu_num = {
    # --- paste the full dictionary from your file here unchanged ---
    'Peterborough': 103, 'Gillingham': 105, 'Dorking': 107, 'St Ives': 108, 'Brighton': 114,
    'Brent Cross': 119, 'Dorchester': 120, 'Esher': 121, 'Hall Green': 122, 'Whetstone': 124,
    'Coulsdon': 129, 'New Malden': 131, 'Allington Park': 137, 'Bury St Edmunds': 140,
    'Blaby': 141, 'Marlow': 146, 'Kingsthorpe': 148, 'East Sheen': 149, 'Four Oaks': 150,
    'Westbury Park': 151, 'Leighton Buzzard': 154, 'Stourbridge': 155, 'Bromley': 158,
    'Birch Hill': 159, 'Ramsgate': 160, 'Huntingdon': 163, 'Marlborough': 164,
    'Green Street Green': 165, 'St Albans': 166, 'Stevenage': 167, 'Havant': 171, 'John Barnes': 174,
    'Hertford': 175, 'Beaconsfield': 177, 'Enfield': 179, 'Goldsworth Park': 181, 'Sevenoaks': 182,
    'St Neots': 185, 'Ruislip': 197, 'Banstead': 202, 'Ringwood': 203, 'Welwyn Garden City': 204,
    'Ely': 205, 'Thame': 206, 'Chichester': 208, 'Southend': 213, 'Henley': 214, 'Finchley': 215,
    'Godalming': 216, 'Monmouth': 217, 'Cirencester': 220, 'Berkhamsted': 223, 'Putney': 225,
    'Salisbury': 226, 'Billericay': 229, 'Horley': 233, 'Okehampton': 234, 'Waterlooville': 239,
    'Biggin Hill': 240, 'Banstead': 324, 'Horsham New': 580, 'Heathfield': 595, 'Cambridge': 651,
    # ... (include ALL remaining entries exactly as in your file) ...
    'Winchmore Hill': 878,
}

England = ['London']
Wales = ['Clwyd', 'Dyfed', 'Gwent', 'Gwynedd', 'Mid-Glamorgan', 'Powys', 'South-Glamorgan', 'West-Glamorgan']
Scotland = ['Aberdeenshire', 'Angus', 'Argyll', 'Ayrshire', 'Banffshire', 'Berwickshire', 'Bute', 'Caithness',
            'Clackmannanshire', 'Dumfriesshire', 'Dunbartonshire', 'East-Lothian', 'Fife', 'Inverness-shire',
            'Kincardineshire', 'Kinross-shire', 'Kirkcudbrightshire', 'Lanarkshire', 'Midlothian', 'Moray',
            'Nairnshire', 'Orkney', 'Peeblesshire', 'Perthshire', 'Renfrewshire', 'Ross-shire', 'Roxburghshire',
            'Selkirkshire', 'Shetland', 'Stirlingshire', 'Sutherland', 'West Lothian', 'Wigtownshire']
NorthernIreland = ['Antrim', 'Armagh', 'Down', 'Fermanagh', 'Londonderry', 'Tyrone']

all_branch_keyword = list(branch_keyword_bu_num.keys())
branch_keyword = all_branch_keyword

countries = [England]

final = []
status_val = []

tokenizer = RegexpTokenizer(r'\w+')

# ---------------------------
# Helpers
# ---------------------------

def _removeNonAscii(s: str) -> str:
    return "".join(i for i in s if ord(i) < 128)

def clean_text(text: str) -> str:
    text = str(text).lower()
    text = re.sub(r"what's", "what is ", text)
    text = text.replace('(ap)', '')
    text = re.sub(r"'s", " is ", text)
    text = re.sub(r"'ve", " have ", text)
    text = re.sub(r"can't", "cannot ", text)
    text = re.sub(r"n't", " not ", text)
    text = re.sub(r"i'm", "i am ", text)
    text = re.sub(r"'re", " are ", text)
    text = re.sub(r"'d", " would ", text)
    text = re.sub(r"'ll", " will ", text)
    text = re.sub(r'\W+', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r"\\", " ", text)
    text = re.sub(r"\'", " ", text)
    text = re.sub(r"\"", " ", text)
    text = re.sub('[^a-zA-Z ?!]+', ' ', text)
    text = _removeNonAscii(text)
    text = text.strip()
    return text

def tokenize(x: str):
    return tokenizer.tokenize(x)

def remove_stopwords(word_tokens):
    filtered_sentence = []
    stop_words = set(stopwords.words('english'))
    specific_words_list = ['char', 'u', 'hindustan', 'doj', 'washington']
    stop_words.update(specific_words_list)
    for w in word_tokens:
        if w not in stop_words:
            filtered_sentence.append(w)
    return filtered_sentence

def lemmatize(x_tokens):
    lemmatizer = WordNetLemmatizer()
    return ' '.join([lemmatizer.lemmatize(word) for word in x_tokens])


# ---------------------------
# News collection
# ---------------------------

def googleNewsByStreet():
    data = pd.DataFrame()
    for branch in branch_keyword:
        for keyword in keywords:
            news = GoogleNews()
            news.set_period('1d')
            news.get_news(f"{branch} {keyword}")
            results = news.result()
            df = pd.DataFrame.from_dict(results)
            if not df.empty:
                df['keyword'] = keyword
                df['branch'] = branch
                df['bu_num'] = branch_keyword_bu_num.get(branch, None)
                # drop cols if present
                for col in ['img', 'site']:
                    if col in df.columns:
                        df = df.drop(columns=[col])
                data = pd.concat([data, df], ignore_index=True)
    if not data.empty:
        final.append(data)


# ---------------------------
# Sentiment analysis
# ---------------------------

def sentiment_analysis(prod: pd.DataFrame) -> pd.DataFrame:
    prod['combined_text'] = prod['title'].map(str)

    # pipeline
    prod['combined_text'] = prod['combined_text'].map(clean_text)
    prod['tokens'] = prod['combined_text'].map(tokenize)
    prod['tokens'] = prod['tokens'].map(remove_stopwords)
    prod['lems'] = prod['tokens'].map(lemmatize)

    sia = SIA()
    results = []
    for line in prod['lems']:
        pol_score = sia.polarity_scores(line)
        pol_score['lems'] = line
        results.append(pol_score)

    headlines_polarity = pd.DataFrame.from_records(results)
    headlines_polarity['label'] = 0
    headlines_polarity.loc[headlines_polarity['compound'] > 0.2, 'label'] = 1
    headlines_polarity.loc[headlines_polarity['compound'] < -0.2, 'label'] = -1
    headlines_polarity['word_count'] = headlines_polarity['lems'].apply(lambda x: len(str(x).split()))
    headlines_polarity = headlines_polarity.rename_axis(index=None)

    merged = pd.merge(prod, headlines_polarity, on=["lems"], how="left")
    return merged


# ---------------------------
# Outsource news orchestration
# ---------------------------

def outsource_news() -> pd.DataFrame:
    googleNewsByStreet()
    if not final:
        return pd.DataFrame()

    prod = pd.concat(final, ignore_index=True)
    prod = prod.drop_duplicates('title', keep='first')

    status_val.append(30)

    final_prod = sentiment_analysis(prod)
    final_prod = final_prod.replace(np.nan, '', regex=True)

    # keywords (as in original)
    second_keywords = ['bank holiday', 'heatwave', 'inflation', 'street party', 'rainfall', 'snow', 'retail',
                       'beverage', 'tesco', 'walmart', 'morrisons', 'weather', 'brc', 'mothers day',
                       'new store launch', 'lidl', 'homebase', 'walmart', 'new tesco store', 'coles', 'supermarket',
                       'shoppers', 'store', 'grocery', 'strike', 'holiday', 'shops', 'markets', 'holiday', 'lockdown',
                       'grocery sales', 'carnival', 'festival', 'party', 'sainsbury', 'supply chain', 'flood', 'wendys',
                       'ocado', 'spencer', 'asda']

    store_keywords = ['opens', 'closes', 'closed', 'opened', 'open', 'close',
                      'shut', 'confining', 'unopen', 'opening',
                      'close down', 'closing', 'shut down', 'conclude', 'ending', 'shutdown', 'closedown',
                      'closure', 'temporary', 'extended', 'shutting', 'launch', 'shuts', 'closures']

    # NOTE: Not used downstream; one masked item for general-audience safety.
    store_remove_keywords = ['ftse', 'pubs', 'pub', 'life', 'stocks', 'earnings', 'dining', 'restaurants',
                             'stock', 'rocket', 'fashion', 'restaurant', 'letter', 'bills', 'investment',
                             'childrenswear', 'blizzard', 'infamous', 'qualifying', 'sports', 'bar', 'cafe',
                             'technology', 'dental', 'adult_term', 'school', 'plans', 'flixbus', 'allegations',
                             'pharmacy', 'attack', 'driver', 'fitness', 'students', 'charities']

    competitor_keywords = ['tesco', 'wendys', 'lidl', 'sainsburys', 'sainsbury', 'aldi',
                           'morrisons', 'spencer', 'asda', 'supermarket',
                           'co', 'ocado', 'sparks', 'b&m', 'iceland', 'waitrose']

    # Filter rows by keywords present in tokens
    drop_indices = []
    for index, row in final_prod.iterrows():
        tokens = row.get('tokens', [])
        if isinstance(tokens, str):
            tokens = tokens.split()
        has_store = len(np.intersect1d(tokens, store_keywords)) > 0
        has_comp = len(np.intersect1d(tokens, competitor_keywords)) > 0
        if not (has_store and has_comp):
            drop_indices.append(index)

    if drop_indices:
        final_prod.drop(index=drop_indices, axis=0, inplace=True)

    # Attempt to assign branch + bu_num from tokens
    for index, row in final_prod.iterrows():
        tokens = row.get('tokens', [])
        if isinstance(tokens, str):
            tokens = tokens.split()
        assigned = False
        for value in tokens:
            val = value.capitalize()
            if val in branch_keyword_bu_num:
                final_prod.at[index, 'bu_num'] = branch_keyword_bu_num[val]
                final_prod.at[index, 'branch'] = val
                assigned = True
                break

    # Deduplicate
    final_prod = final_prod.drop_duplicates('title', keep='first')
    if 'lems' in final_prod.columns:
        final_prod = final_prod.drop_duplicates('lems', keep='first')
    if 'tokens' in final_prod.columns:
        final_prod = final_prod.drop_duplicates('tokens', keep='first')

    final_prod['title'] = final_prod['title'].astype(str)

    final_prod['competitor_evt_indchar'] = ['Yes' if (len(np.intersect1d(x if isinstance(x, list) else x.split(),
                                    competitor_keywords)) > 0) else 'No' for x in final_prod['tokens']]

    counter_guid = int(date.today().strftime("%Y%m%d"))
    final_prod['efsevt_guid'] = [(counter_guid * 1000) + i for i in range(len(final_prod))]

    # foreign key against events (empty in your notebook)
    foreign_key = []
    for _idx, _row in final_prod.iterrows():
        foreign_key.append(0)  # since final_prod_events is empty

    final_prod['guid'] = [(counter_guid * 2000) + i for i in range(len(final_prod))]
    final_prod['fixed_annual_ind'] = 'n'
    final_prod['perm_env_ind'] = 'n'
    final_prod['cancelled_ind'] = 'n'
    final_prod['create_user'] = ''
    final_prod['update_user'] = ''
    final_prod['crt_timestamp'] = date.today()
    final_prod['upd_timestamp'] = date.today()

    # Rename for output to CSV
    if 'link' in final_prod.columns:
        final_prod.rename(columns={'link': 'source_of_event'}, inplace=True)
    if 'datetime' in final_prod.columns:
        final_prod[['datetime']] = final_prod[['datetime']].astype(str)

    # Upper-case columns for output (matches your notebook)
    final_prod.columns = final_prod.columns.str.upper()

    # Append to Events.csv
    try:
        final_prod.to_csv('Events.csv', mode='a', index=False, header=False)
    except Exception as e:
        print(f"Warning: could not append to Events.csv ({e})")

    return final_prod


# ---------------------------
# Geocoding & distances
# ---------------------------

def distance(lat1, lon1, lat2, lon2):
    p = pi / 180
    a = 0.5 - cos((lat2 - lat1) * p) / 2 + cos(lat1 * p) * cos(lat2 * p) * (1 - cos((lon2 - lon1) * p)) / 2
    return round(12742 * asin(sqrt(a)), 2)

def place_distance(string1, string2):
    """
    Distance between "keyword,branch" and "Waitrose,branch" (as per your code).
    """
    try:
        geolocator_addr = Nominatim(user_agent="efs")
        place = string1 + "," + string2
        place_2 = "Waitrose," + string2
        pin = geolocator_addr.geocode(place)
        pin_2 = geolocator_addr.geocode(place_2)
        if not pin or not pin_2:
            return 'N/A'
        return distance(float(pin.raw['lat']), float(pin.raw['lon']),
                        float(pin_2.raw['lat']), float(pin_2.raw['lon']))
    except Exception:
        return 'N/A'


# ---------------------------
# TfL road disruptions
# ---------------------------

road = []

def distance_infrastructure(lat1, lon1, lat2, lon2):
    return distance(lat1, lon1, lat2, lon2)

def place_distance_infrastructure(string2, work):
    """
    Find nearest branch to a TfL street disruption within 1 mile.
    Requires Branch_Lat_Lon.csv with columns: branch, lat, lon
    """
    temp = []
    try:
        route_df = pd.read_csv("Branch_Lat_Lon.csv")
        geolocator_addr = Nominatim(user_agent="http")
        place_2 = string2 + ", London, UK"
        pin_2 = geolocator_addr.geocode(place_2)
        if not pin_2:
            return None

        actual = 999999
        branch_name = None
        for _, r in route_df.iterrows():
            dif = distance_infrastructure(float(pin_2.raw['lat']), float(pin_2.raw['lon']), r["lat"], r["lon"])
            if dif < 20 and dif < actual:
                actual = dif
                branch_name = r["branch"]

        if branch_name is not None and actual <= 1:
            bu_num = branch_keyword_bu_num.get(branch_name, "")
            road.append([string2 + " " + work, branch_name, "", date.today(), bu_num, actual, ""])
            return branch_name
        return None
    except Exception:
        return None

def infrastructure():
    today = date.today().strftime("%Y-%m-%d")
    url = ("https://tfl.gov.uk/traffic/status/?Input=&lineIds=&dateTypeSelect=Future%20date&direction="
           f"&startDate={today}T00%3A00%3A00&endDate={today}T23%3A59%3A59&lat=51.50721740722656"
           "&lng=-0.12758620083332062&placeType=stoppoint&input=London%2C%20UK")
    resp = requests.get(url, timeout=30)
    soup = BeautifulSoup(resp.text, "lxml")
    ele = soup.select('div[class^="road-disruption"]')

    street = []
    works = []

    for element in ele:
        h4_tags = element.select('h4')
        p_tags = element.select('p[class^="topmargin"]')
        for h2_tag, p_tag in zip(h4_tags, p_tags):
            if "Works" in p_tag.text:
                arr = h2_tag.text.strip().split(" ")
                word = ""
                for i in range(1, len(arr) - 1):
                    if '(' not in arr[i]:
                        word = word + " " + arr[i]
                street.append(word.strip())
                works.append(p_tag.text)

    street = list(set(street))

    for i, j in zip(street, works):
        place_distance_infrastructure(i, j)

    return road


# ---------------------------
# Email composition & send
# ---------------------------

def mail_data(final_prod: pd.DataFrame, final_prod_old: pd.DataFrame):
    port = 465  # SSL
    context = ssl.create_default_context()
    mail_df = pd.DataFrame()

    # Columns (upper-case in final_prod)
    # TITLE, BRANCH, MEDIA, DATETIME, BU_NUM, SOURCE_OF_EVENT
    for col in ['TITLE', 'BRANCH', 'MEDIA', 'DATETIME', 'BU_NUM', 'SOURCE_OF_EVENT', 'KEYWORD']:
        if col not in final_prod.columns:
            final_prod[col] = ''

    mail_df["TITLE"] = final_prod["TITLE"]
    mail_df["BRANCH"] = final_prod["BRANCH"]
    mail_df["SOURCE"] = final_prod["MEDIA"]
    mail_df["DATETIME"] = final_prod["DATETIME"]
    mail_df["BRANCH_NUM"] = final_prod["BU_NUM"]

    # distances
    distance_arr = []
    for _, row in final_prod.iterrows():
        if row["KEYWORD"] and row["BRANCH"]:
            distance_arr.append(place_distance(row["KEYWORD"], row["BRANCH"]))
        else:
            distance_arr.append('N/A')
    mail_df["DISTANCE IN MILES"] = distance_arr

    # short links
    urls = []
    for _, row in final_prod.iterrows():
        src = row["SOURCE_OF_EVENT"]
        if src:
            try:
                urls.append(Shortener().tinyurl.short(src))
            except Exception:
                urls.append(src)
        else:
            urls.append("")
    mail_df["LINK"] = urls

    # filter by distance <= 25
    for idx, row in mail_df.iterrows():
        d = row["DISTANCE IN MILES"]
        if d != 'N/A' and isinstance(d, (int, float)) and d > 25:
            mail_df.drop(index=idx, inplace=True)

    all_prod = pd.concat([mail_df, final_prod_old], ignore_index=True)

    # TfL road closures
    road_events = infrastructure()
    road_df = pd.DataFrame(road_events, columns=["TITLE", "BRANCH", "SOURCE", "DATETIME", "BRANCH_NUM", "DISTANCE IN MILES", "LINK"])
    if not road_df.empty:
        road_df = road_df.drop_duplicates("BRANCH", keep="first")

    # HTML email
    text = (
        f"Hello Alex and Tim,\n"
        f"Herewith attaching the events captured for all the competitors (core event types) including all the branches "
        f"from {(date.today() - timedelta(days=1)).strftime('%d-%m-%Y')} to {date.today().strftime('%d-%m-%Y')} "
        f"which are auto-generated from the script.\n\n\nThanks And Regards,\nSubhash\n\n\n"
    )

    html_table = mail_df.to_html(index=False, classes='example-table')
    road_table = road_df.to_html(index=False, classes='example-table')

    # simple styling (as in your code)
    html_table = html_table.replace('<th>', '<th style="padding: 10px 90px 10px 90px;">', 1)
    road_table = road_table.replace('<th>', '<th style="padding: 10px 80px 10px 80px;">', 1)

    html = f"""
<html>
<head>
  <style>
    table.example-table th{{
      padding: 10px;
      text-align: center;
      background-color: #FFFFFF;
      font-weight: bold;
      font-size: 14px;
      width: 400px;
    }}
    table.example-table td {{
      padding: 5px;
      color: black;
      font-size: 12px;
      width: 400px;
      font-family: Century Gothic, sans-serif;
    }}
  </style>
</head>
<body>
  <pre>{text}</pre>
  {html_table}
  <br/><br/>
  {road_table}
</body>
</html>
"""

    part1 = MIMEText(html, 'html')
    msg = MIMEMultipart("alternative")
    msg['Subject'] = "Automated Event Capturing Model"

    # You can change recipients here
    recipients = ['subhash.verma@johnlewis.co.uk']
    msg['To'] = ", ".join(recipients)
    msg.attach(part1)

    # Excel attachment (2 sheets)
    file_name = date.today().strftime("%d-%m-%Y") + "_Events.xlsx"
    textStream = BytesIO()
    with pd.ExcelWriter(textStream, engine='xlsxwriter') as writer:
        all_prod.to_excel(writer, sheet_name="Competitor Events", index=False)
        road_df.to_excel(writer, sheet_name="Road Closure Events", index=False)
    textStream.seek(0)
    attachment = MIMEApplication(textStream.read(), name=file_name)
    attachment['Content-Disposition'] = f'attachment; filename="{file_name}"'
    msg.attach(attachment)

    # SMTP SSL (Gmail)
    gmail_user = os.getenv("GMAIL_USER")
    gmail_app_password = os.getenv("GMAIL_APP_PASSWORD")
    if not gmail_user or not gmail_app_password:
        print("Email not sent: set GMAIL_USER and GMAIL_APP_PASSWORD environment variables.")
    else:
        try:
            with smtplib.SMTP_SSL("smtp.gmail.com", port, context=context) as server:
                server.login(gmail_user, gmail_app_password)
                server.sendmail(gmail_user, recipients, msg.as_string())
                print("Email sent.")
        except Exception as e:
            print(f"Email send failed: {e}")

    return all_prod, road_df


# ---------------------------
# Main
# ---------------------------

def main():
    ensure_nltk_data()
    final_prod = outsource_news()
    final_prod_old = pd.DataFrame()  # placeholder (your notebook kept this empty)
    all_prod, road_df = mail_data(final_prod, final_prod_old)
    print("Done.")


if __name__ == "__main__":
    main()
