{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install feedparser\n",
    "!pip install GoogleNews\n",
    "!pip install gnewsclient\n",
    "!pip install snscrape==0.6.2.20230320\n",
    "!pip install rake_nltk\n",
    "!pip install geopy\n",
    "!pip install pgeocode\n",
    "!pip install requests lxml\n",
    "!pip install beautifulsoup4\n",
    "!pip install pretty-html-table\n",
    "!pip install pyshorteners\n",
    "!pip install xlsxwriter\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "# from newsapi import NewsApiClient\n",
    "import requests\n",
    "from GoogleNews import GoogleNews\n",
    "from gnewsclient import gnewsclient\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "from datetime import datetime, date, timedelta\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from rake_nltk import Rake\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "# NEW: RSS-based fetching\n",
    "import feedparser\n",
    "from urllib.parse import quote_plus\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "start_date = []\n",
    "end_date = []\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sources = [\"bbc-news\", \"the-telegraph\", \"the-guardian-uk\", \"cnn\", \"abc-news-au\",\n",
    "           \"dailymail.co.uk\", \"metro.co.uk\", \"mirror.co.uk\", \"news.google.com\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "all_keywords = ['strike', 'holiday', 'lockdown',\n",
    " 'inflation', 'grocery sales', 'carnival', 'festival', 'party', 'Walmart', \"Tesco\", \"Sainsbury's\", \"supply chain\", \"flood\", \"wendys\", \"lidl\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "keywords = ['Lidl','Waitrose','Tesco','Walmart','Sainsbury\\'s', 'Aldi', 'Asda', 'Marks & Spencers', 'Morrison\\'s']\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "events = ['autumn bank holiday']\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "all_events = ['autumn bank holiday']\n",
    "\n",
    "final_prod_events = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "counter = 6000\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "gnews_client_topics = ['Top Stories','World','Nation','Business','Technology','Entertainment','Sports','Science','Health']\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "branch_keyword_bu_num = {\n",
    "'Peterborough': 103,\n",
    "'Gillingham': 105,\n",
    "'Dorking': 107,\n",
    "'St Ives': 108,\n",
    "'Brighton': 114,\n",
    "'Brent Cross': 119,\n",
    "'Dorchester': 120,\n",
    "'Esher': 121,\n",
    "'Hall Green': 122,\n",
    "'Whetstone': 124,\n",
    "'Coulsdon': 129,\n",
    "'New Malden': 131,\n",
    "'Allington Park': 137,\n",
    "'Bury St Edmunds': 140,\n",
    "'Blaby': 141,\n",
    "'Marlow': 146,\n",
    "'Kingsthorpe': 148,\n",
    "'East Sheen': 149,\n",
    "'Four Oaks': 150,\n",
    "'Westbury Park': 151,\n",
    "'Leighton Buzzard': 154,\n",
    "'Stourbridge': 155,\n",
    "'Bromley': 158,\n",
    "'Birch Hill': 159,\n",
    "'Ramsgate': 160,\n",
    "'Huntingdon': 163,\n",
    "'Marlborough': 164,\n",
    "'Green Street Green': 165,\n",
    "'St Albans': 166,\n",
    "'Stevenage': 167,\n",
    "'Havant': 171,\n",
    "'John Barnes': 174,\n",
    "'Hertford': 175,\n",
    "'Beaconsfield': 177,\n",
    "'Enfield': 179,\n",
    "'Goldsworth Park': 181,\n",
    "'Sevenoaks': 182,\n",
    "'St Neots': 185,\n",
    "'Ruislip': 197,\n",
    "'Banstead': 202,\n",
    "'Ringwood': 203,\n",
    "'Welwyn Garden City': 204,\n",
    "'Ely': 205,\n",
    "'Thame': 206,\n",
    "'Chichester': 208,\n",
    "'Southend': 213,\n",
    "'Henley': 214,\n",
    "'Finchley': 215,\n",
    "'Godalming': 216,\n",
    "'Monmouth': 217,\n",
    "'Cirencester': 220,\n",
    "'Berkhamsted': 223,\n",
    "'Putney': 225,\n",
    "'Salisbury': 226,\n",
    "'Billericay': 229,\n",
    "'Horley': 233,\n",
    "'Okehampton': 234,\n",
    "'Waterlooville': 239,\n",
    "'Biggin Hill': 240,\n",
    "'Banstead': 324,\n",
    "'Horsham New': 580,\n",
    "'Heathfield': 595,\n",
    "'Cambridge': 651,\n",
    "'Hailsham': 653,\n",
    "'Hythe': 654,\n",
    "'Paddock Wood': 655,\n",
    "'Saltash': 656,\n",
    "'Sidmouth': 657,\n",
    "'Sudbury': 658,\n",
    "'Thatcham': 659,\n",
    "'Worcester Park': 661,\n",
    "'Wymondham': 662,\n",
    "'Cheltenham': 663,\n",
    "'Belgravia': 665,\n",
    "'Tonbridge': 667,\n",
    "'Chandlers Ford': 668,\n",
    "'Portishead': 669,\n",
    "'Romsey': 671,\n",
    "'Wandsworth': 673,\n",
    "'Newmarket': 674,\n",
    "'Sandbach': 680,\n",
    "'Fulham': 681,\n",
    "'Towcester': 682,\n",
    "'Abergavenny': 683,\n",
    "'Hitchin': 685,\n",
    "'Swaffham': 686,\n",
    "'Newport': 687,\n",
    "'Barry': 688,\n",
    "'Worthing': 689,\n",
    "'Otley': 691,\n",
    "'Farnham': 692,\n",
    "'Dartford': 693,\n",
    "'Sheffield': 695,\n",
    "'Wolverhampton': 696,\n",
    "'Willerby': 697,\n",
    "'Lichfield': 699,\n",
    "'Wilmslow': 711,\n",
    "'Lewes': 727,\n",
    "'East Grinstead': 741,\n",
    "'Buxton': 748,\n",
    "'St Katharine Docks': 753,\n",
    "'West Ealing': 764,\n",
    "'Hersham': 765,\n",
    "'Bishop s Stortford': 101,\n",
    "'Buckhurst Hill': 102,\n",
    "'Epsom': 104,\n",
    "'Longfield': 109,\n",
    "'Crowborough': 110,\n",
    "'Holloway Road': 112,\n",
    "'Milton Keynes': 115,\n",
    "'Dibden': 118,\n",
    "'Burgess Hill': 123,\n",
    "'Temple Fortune': 125,\n",
    "'Saffron Walden': 135,\n",
    "'Evington': 136,\n",
    "'Witney': 142,\n",
    "'Harrow Weald': 143,\n",
    "'Gosport': 152,\n",
    "'Wantage': 153,\n",
    "'Daventry': 156,\n",
    "'Weybridge': 157,\n",
    "'Winton': 161,\n",
    "'Andover': 168,\n",
    "'Southsea': 170,\n",
    "'Kings Road': 173,\n",
    "'Cobham': 176,\n",
    "'Caterham': 178,\n",
    "'Woodley': 180,\n",
    "'Harpenden': 183,\n",
    "'Caversham': 184,\n",
    "'Northwood': 186,\n",
    "'Richmond': 188,\n",
    "'West Byfleet': 189,\n",
    "'Sunningdale': 190,\n",
    "'Barnet': 191,\n",
    "'Chesham': 192,\n",
    "'Bath': 193,\n",
    "'Maidenhead': 194,\n",
    "'Kingston': 195,\n",
    "'Fleet': 196,\n",
    "'Yateley': 198,\n",
    "'Horsham': 200,\n",
    "'Tenterden': 201,\n",
    "'Bloomsbury': 207,\n",
    "'Petersfield': 209,\n",
    "'Stroud': 210,\n",
    "'Abingdon': 211,\n",
    "'Beckenham': 212,\n",
    "'South Harrow': 219,\n",
    "'Wokingham': 221,\n",
    "'Norwich': 222,\n",
    "'Bromley South': 224,\n",
    "'Newark': 227,\n",
    "'Gloucester Road': 230,\n",
    "'South Woodford': 231,\n",
    "'Surbiton': 232,\n",
    "'Staines': 235,\n",
    " 'Marylebone': 236,\n",
    "'Great Malvern': 237,\n",
    "'Twyford': 238,\n",
    "'Byres Road': 308,\n",
    "'Weston Super Mare': 309,\n",
    "'Wellington': 315,\n",
    "'Ashbourne': 316,\n",
    "'Storrington': 317,\n",
    "'Menai Bridge': 318,\n",
    "'Melksham': 319,\n",
    "'Colchester': 455,\n",
    "'JL Foodhall Oxford Street': 456,\n",
    "'Pontprennau': 457,\n",
    "'Crewkerne': 458,\n",
    "'Kenilworth': 460,\n",
    "'Eldon Square': 461,\n",
    "'Westfield London': 462,\n",
    "'Winchester': 463,\n",
    "'Alcester': 474,\n",
    "'Bridport': 475,\n",
    "'Caldicot': 476,\n",
    "'Croydon': 477,\n",
    "'Haslemere': 478,\n",
    "'Headington': 479,\n",
    "'Holsworthy': 480,\n",
    "'Leigh On Sea': 481,\n",
    "'Ponteland': 482,\n",
    "'Saxmundham': 483,\n",
    "'Stamford': 484,\n",
    "'Torquay': 485,\n",
    "'Upminster': 486,\n",
    "'Lutterworth': 487,\n",
    "'Clerkenwell': 492,\n",
    "'JL Foodhall Bluewater': 493,\n",
    "'Altrincham': 494,\n",
    "'Frimley': 652,\n",
    "'Twickenham': 660,\n",
    "'Canary Wharf': 664,\n",
    "'Mill Hill': 670,\n",
    "'Droitwich': 672,\n",
    "'Wallingford': 675,\n",
    "'Newbury': 676,\n",
    "'Sanderstead': 677,\n",
    "'Kensington': 678,\n",
    "'Harrogate': 684,\n",
    "'Rushden': 690,\n",
    "'Lincoln': 694,\n",
    "'Rickmansworth': 698,\n",
    "'Ashford': 705,\n",
    "'Cheadle Hulme': 710,\n",
    "'Balham': 719,\n",
    "'Southampton New': 720,\n",
    "'Ampthill': 722,\n",
    "'Durham': 730,\n",
    "'Barbican': 732,\n",
    "'Formby': 749,\n",
    "'Comely Bank': 750,\n",
    "'Christchurch': 754,\n",
    "'Bayswater': 756,\n",
    "'Eastbourne': 757,\n",
    "'Chiswick': 760,\n",
    "'Morningside': 761,\n",
    "'Parkstone': 766,\n",
    "'Clapham Junction': 767,\n",
    "'Edgware Road': 768,\n",
    "'Buckingham': 769,\n",
    "'Windsor New': 772,\n",
    "'Islington': 780,\n",
    "'Hexham': 782,\n",
    "'Harborne': 796,\n",
    "'Brackley': 797,\n",
    "'Lymington New': 798,\n",
    "'Sandhurst': 799,\n",
    "'Trinity Square': 833,\n",
    "'Clifton': 834,\n",
    "'Crouch End': 835,\n",
    "'Oxted': 838,\n",
    "'Enfield CFC': 199,\n",
    "'Greenford CFC': 259,\n",
    "'Evesham': 303,\n",
    "'York': 311,\n",
    "'Poynton': 312,\n",
    "'East Cowes': 313,\n",
    "'Wimbledon': 314,\n",
    "'Knutsford': 326,\n",
    "'Newton Mearns': 327,\n",
    "'Stratford City': 328,\n",
    "'Alton': 329,\n",
    "'St Saviour (Jersey)': 332,\n",
    "'Rohais (Guernsey)': 333,\n",
    "'St Helier (Jersey)': 334,\n",
    "'Admiral Park (Guernsey)': 335,\n",
    "'Red Houses (Jersey)': 336,\n",
    "'MOUNTSORREL': 403,\n",
    "'Gerrards Cross': 459,\n",
    "'Sevenoaks': 464,\n",
    "'Marlow': 465,\n",
    "'Cardiff Queen Street': 501,\n",
    "'Acton': 502,\n",
    " 'Swindon': 504,\n",
    "'Littlehampton': 505,\n",
    "'Uckfield': 506,\n",
    "'Hereford': 507,\n",
    "'Malmesbury': 511,\n",
    "'Coulsdon DFC': 513,\n",
    "'Bagshot': 514,\n",
    "'Nailsea': 515,\n",
    "'Parsons Green': 516,\n",
    "'Egham': 519,\n",
    "'Jesmond': 520,\n",
    "'Enfield Chase': 521,\n",
    "'Sutton Coldfield': 522,\n",
    "'Chippenham': 523,\n",
    "'West Hampstead': 524,\n",
    "'Shrewsbury': 525,\n",
    "'Tottenham Court Road': 526,\n",
    "'Dorking': 527,\n",
    "'Wimbledon Hill': 528,\n",
    "'Hawkhurst': 529,\n",
    "'Fulham Palace Road': 530,\n",
    "'Peterborough': 531,\n",
    "'Canterbury': 533,\n",
    "'Sceptre (Watford)': 534,\n",
    "'Kensington Gardens': 535,\n",
    "'Camden': 536,\n",
    "'Addlestone': 542,\n",
    "'Fitzroy Street': 552,\n",
    "'Teignmouth': 554,\n",
    "'Hornchurch': 555,\n",
    "'Edenbridge': 556,\n",
    "'Keynsham': 557,\n",
    "'Spinningfields': 558,\n",
    "'Cheam': 559,\n",
    "'Alderley Edge': 560,\n",
    "'Walton-on-Thames': 562,\n",
    "'Locks Heath': 563,\n",
    "'Burgh Heath': 567,\n",
    "'Petts Wood': 568,\n",
    "'Portman Square': 569,\n",
    "'Burnt Common': 571,\n",
    "'Walbrook': 573,\n",
    "'Leeds': 574,\n",
    "'Broxbourne': 575,\n",
    "'Amersham': 578,\n",
    "'Bayswater Temp': 579,\n",
    "'Oxford Botley Road': 581,\n",
    "'BASINGSTOKE': 582,\n",
    "'Old Brompton Road': 583,\n",
    "'Hazlemere': 584,\n",
    "'Ealing': 586,\n",
    "'West Kensington': 587,\n",
    "'Palmers Green': 588,\n",
    "'Guildford': 589,\n",
    "'Kings Cross': 590,\n",
    "'Wollaton': 591,\n",
    "'Rustington': 596,\n",
    "'BATTERSEA NINE ELMS': 598,\n",
    "'UTTOXETER': 599,\n",
    "'High Holborn': 601,\n",
    "'Alderley Old': 602,\n",
    "'Sherborne': 604,\n",
    "'Hove': 605,\n",
    "'Leek': 606,\n",
    "'High Wycombe': 607,\n",
    "'Hampton': 612,\n",
    "'Pimlico': 614,\n",
    "'Foregate Street': 615,\n",
    "'Clapham Common': 616,\n",
    "'Kings Cross Station': 619,\n",
    "'Stirling': 620,\n",
    "'North Walsham': 622,\n",
    "'Aylesbury': 625,\n",
    "'Milngavie': 630,\n",
    "'Ipswich': 632,\n",
    "'Manchester Piccadilly': 636,\n",
    "'Highbury Corner': 637,\n",
    " 'Muswell Hill': 639,\n",
    "'Knightsbridge': 641,\n",
    "'Solihull': 642,\n",
    "'Sidcup': 643,\n",
    "'Notting Hill Gate': 644,\n",
    "'Truro': 648,\n",
    "'Worcester': 700,\n",
    "'Warminster': 701,\n",
    "'Exeter': 702,\n",
    "'South Bank Tower': 703,\n",
    "'Bracknell': 706,\n",
    "'Stratford Upon Avon': 708,\n",
    "'Walton-le-Dale': 721,\n",
    "'Bedford': 725,\n",
    "'Wootton': 726,\n",
    "'Market Harborough': 728,\n",
    "'Poundbury': 733,\n",
    "'Cowbridge': 735,\n",
    "'ROEHAMPTON': 736,\n",
    "'Battersea': 737,\n",
    "'Bagshot Road': 738,\n",
    "'Tubs Hill': 739,\n",
    "'Greenwich': 740,\n",
    "'Colmore Row (Birmingham)': 742,\n",
    "'Ipswich (Corn Exchange)': 743,\n",
    "'Kings Hill': 744,\n",
    "'Chipping Sodbury': 751,\n",
    "'Oakgrove': 752,\n",
    "'Dorking': 755,\n",
    "'Oundle': 758,\n",
    "'Northwich': 759,\n",
    "'Helensburgh': 771,\n",
    "'Monument': 773,\n",
    "'Little Waitrose at John Lewis Watford': 781,\n",
    "'Victoria Street': 783,\n",
    "'Vauxhall': 789,\n",
    "'Horley - Brighton Road': 802,\n",
    "'Wimborne': 805,\n",
    "'Headington - London Road': 806,\n",
    "'Guildford Worplesdon Road': 808,\n",
    "'Little Waitrose John Lewis Southampton': 815,\n",
    "'East Putney': 820,\n",
    "'Meanwood': 828,\n",
    "'Chester': 842,\n",
    "'Raynes Park': 846,\n",
    "'Oadby': 847,\n",
    "'Leatherhead': 859,\n",
    "'Victoria Bressenden Place': 860,\n",
    "'SKY (OSTERLEY)': 865,\n",
    "'Faringdon': 871,\n",
    "'Haywards Heath': 873,\n",
    "'Banbury': 874,\n",
    "'Finchley Central': 876,\n",
    "'Bromsgrove': 877,\n",
    "'Winchmore Hill': 878,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# England counties list trimmed to the one you use in code\n",
    "England = ['London']\n",
    "Wales = ['Clwyd', 'Dyfed', 'Gwent', 'Gwynedd', 'Mid-Glamorgan', 'Powys', 'South-Glamorgan', 'West-Glamorgan']\n",
    "Scotland = ['Aberdeenshire', 'Angus', 'Argyll', 'Ayrshire', 'Banffshire', 'Berwickshire', 'Bute', 'Caithness',\n",
    "            'Clackmannanshire', 'Dumfriesshire', 'Dunbartonshire', 'East-Lothian', 'Fife', 'Inverness-shire',\n",
    "            'Kincardineshire', 'Kinross-shire', 'Kirkcudbrightshire', 'Lanarkshire', 'Midlothian', 'Moray', 'Nairnshire',\n",
    "            'Orkney', 'Peeblesshire', 'Perthshire', 'Renfrewshire', 'Ross-shire', 'Roxburghshire', 'Selkirkshire',\n",
    "            'Shetland', 'Stirlingshire', 'Sutherland', 'West Lothian', 'Wigtownshire']\n",
    "NorthernIreland = ['Antrim', 'Armagh', 'Down', 'Fermanagh', 'Londonderry', 'Tyrone']\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "all_branch_keyword = list(branch_keyword_bu_num.keys())\n",
    "branch_keyword = all_branch_keyword\n",
    "countries = [England]\n",
    "final = []\n",
    "status_val = []\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def _removeNonAscii(s):\n",
    "    return \"\".join(i for i in s if ord(i) < 128)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = text.replace('(ap)', '')\n",
    "    text = re.sub(r\"\\'s\", \" is \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r'\\\\W+', ' ', text)\n",
    "    text = re.sub(r'\\\\s+', ' ', text)\n",
    "    text = re.sub(r\"\\\\\\\\\", \"\", text)\n",
    "    text = re.sub(r\"\\'\", \"\", text)\n",
    "    text = re.sub(r'\\\"', '', text)\n",
    "    text = re.sub('[^a-zA-Z ?!]+', '', text)\n",
    "    text = _removeNonAscii(text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(word_tokens):\n",
    "    filtered_sentence = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    specific_words_list = ['char', 'u', 'hindustan', 'doj', 'washington']\n",
    "    stop_words.extend(specific_words_list)\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence\n",
    "\n",
    "def lemmatize(x):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\\\w+')\n",
    "\n",
    "def tokenize(x):\n",
    "    return tokenizer.tokenize(x)\n",
    "\n",
    "nltk.download('all')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def sentiment_analysis(prod):\n",
    "    prod['combined_text'] = prod['title'].map(str)\n",
    "    prod['combined_text'] = prod['combined_text'].map(clean_text)\n",
    "    prod['tokens'] = prod['combined_text'].map(tokenize)\n",
    "    prod['tokens'] = prod['tokens'].map(remove_stopwords)\n",
    "    prod['lems'] = prod['tokens'].map(lemmatize)\n",
    "    sia = SIA()\n",
    "    results = []\n",
    "    for line in prod['lems']:\n",
    "        pol_score = sia.polarity_scores(line)\n",
    "        pol_score['lems'] = line\n",
    "        results.append(pol_score)\n",
    "    headlines_polarity = pd.DataFrame.from_records(results)\n",
    "    headlines_polarity['label'] = 0\n",
    "    headlines_polarity.loc[headlines_polarity['compound'] > 0.2, 'label'] = 1\n",
    "    headlines_polarity.loc[headlines_polarity['compound'] < -0.2, 'label'] = -1\n",
    "    headlines_polarity['word_count'] = headlines_polarity['lems'].apply(lambda x: len(str(x).split()))\n",
    "    headlines_polarity = headlines_polarity.rename_axis(index=None)\n",
    "    return pd.merge(prod, headlines_polarity, on=[\"lems\"], how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def googleNewsByStreet():\n",
    "    # Fetch via Google News RSS for each (branch, keyword) pair and keep expected columns\n",
    "    session = requests.Session()\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    def resolve_google_link(url: str) -> str:\n",
    "        try:\n",
    "            if \"news.google.com\" in url:\n",
    "                r = session.get(url, timeout=20, headers=headers, allow_redirects=True)\n",
    "                return r.url\n",
    "        except Exception:\n",
    "            pass\n",
    "        return url\n",
    "\n",
    "    for branch in branch_keyword:\n",
    "        for keyword in keywords:\n",
    "            query = f\"{branch} {keyword}\"\n",
    "            rss_url = (\n",
    "                f\"https://news.google.com/rss/search?q={quote_plus(query)}+when:1d&hl=en-GB&gl=GB&ceid=GB:en\"\n",
    "            )\n",
    "            try:\n",
    "                resp = session.get(rss_url, timeout=30, headers=headers)\n",
    "                feed = feedparser.parse(resp.content)\n",
    "                rows = []\n",
    "                for entry in feed.entries:\n",
    "                    title = entry.get(\"title\", \"\")\n",
    "                    link = resolve_google_link(entry.get(\"link\", \"\"))\n",
    "                    desc = entry.get(\"summary\", \"\")\n",
    "                    dt = entry.get(\"published\", entry.get(\"updated\", \"\"))\n",
    "                    media = \"\"\n",
    "                    try:\n",
    "                        if hasattr(entry, \"source\") and hasattr(entry.source, \"title\"):\n",
    "                            media = entry.source.title\n",
    "                        elif isinstance(entry.get(\"source\"), dict):\n",
    "                            media = entry[\"source\"].get(\"title\", \"\")\n",
    "                    except Exception:\n",
    "                        media = \"\"\n",
    "                    rows.append({\n",
    "                        \"title\": title,\n",
    "                        \"link\": link,\n",
    "                        \"desc\": desc,\n",
    "                        \"datetime\": dt,\n",
    "                        \"media\": media,\n",
    "                        \"keyword\": keyword,\n",
    "                        \"branch\": branch,\n",
    "                        \"bu_num\": branch_keyword_bu_num.get(branch, None)\n",
    "                    })\n",
    "                df = pd.DataFrame(rows)\n",
    "                if not df.empty:\n",
    "                    data = pd.concat([data, df], ignore_index=True)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] RSS fetch failed for '{query}': {e}\")\n",
    "\n",
    "    data = data.drop(columns=[\"img\", \"site\"], errors=\"ignore\")\n",
    "    final.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def outsource_news():\n",
    "    googleNewsByStreet()\n",
    "    prod = pd.concat(final)\n",
    "    prod = prod.drop_duplicates('title', keep='first')\n",
    "    print(prod)\n",
    "    status_val.append(30)\n",
    "\n",
    "    final_prod = sentiment_analysis(prod)\n",
    "    final_prod = final_prod.replace(np.nan,'',regex=True)\n",
    "\n",
    "    second_keywords = ['bank holiday', 'heatwave', 'inflation', 'street party', 'rainfall', 'snow', 'retail', 'beverage',\n",
    "                       'tesco', 'walmart', 'morrisons', 'weather','brc', 'mothers day', 'new store launch', 'lidl',\n",
    "                       'homebase', 'walmart', 'new tesco store', 'coles', 'supermarket', 'shoppers', 'store', 'grocery',\n",
    "                       'strike', 'holiday','shops', 'markets','holiday', 'lockdown','grocery sales', 'carnival', 'festival',\n",
    "                       'party', 'sainsbury', 'supply chain', 'flood', 'wendys','ocado', 'spencer', 'asda']\n",
    "\n",
    "    remove_keywords = ['accident', 'incident', 'injury', 'political', 'police', 'death', 'traffic', 'lord', 'war', 'actor',\n",
    "                       'movie', 'star', 'lord', 'sex', 'gay','fight', 'crash', 'life', 'plans', 'weapons', 'dating', 'radio', 'tv',\n",
    "                       'guinness', 'husband', 'fashion', 'attack']\n",
    "\n",
    "    store_keywords = ['opens', 'closes', 'closed', 'opened', 'open', 'close','shut', 'confining', 'unopen', 'opening',\n",
    "                      'close down', 'closing', 'shut down', 'conclude', 'ending', 'shutdown', 'closedown','closure', 'temporary',\n",
    "                      'extended', 'shutting', 'launch', 'shuts', 'closures']\n",
    "\n",
    "    store_remove_keywords = ['ftse', 'pubs', 'pub', 'life', 'stocks', 'earnings', 'dining', 'restaurants', 'stock', 'rocket',\n",
    "                              'fashion', 'restaurant','letter', 'bills', 'investment', 'childrenswear', 'blizzard', 'infamous',\n",
    "                              'qualifying', 'sports', 'bar', 'cafe','technology', 'dental', 'boobs', 'school','plans', 'flixbus',\n",
    "                              'allegations', 'pharmacy', 'attack', 'driver', 'fitness', 'students','charities']\n",
    "\n",
    "    competitor_keywords = ['tesco', 'wendys', 'lidl', 'sainsburys', 'sainsbury', 'aldi', 'morrisons', 'spencer', 'asda',\n",
    "                            'supermarket','co', 'ocado', 'sparks', 'b&m', 'iceland', 'waitrose']\n",
    "\n",
    "    print(final_prod)\n",
    "\n",
    "    for index, row in final_prod.iterrows():\n",
    "        if (len(np.intersect1d(row['tokens'], store_keywords)) == 0):\n",
    "            final_prod.drop(index=index, axis=0, inplace=True)\n",
    "        else:\n",
    "            if(len(np.intersect1d(row['tokens'], competitor_keywords)) == 0):\n",
    "                final_prod.drop(index=index, axis=0, inplace=True)\n",
    "\n",
    "    for index, row in final_prod.iterrows():\n",
    "        for value in row['tokens']:\n",
    "            val = value.capitalize()\n",
    "            try:\n",
    "                final_prod.at[index,'bu_num'] = branch_keyword_bu_num[val]\n",
    "                final_prod.at[index,'branch'] = val\n",
    "            except:\n",
    "                n = 0\n",
    "\n",
    "    final_prod = final_prod.drop_duplicates('title', keep='first')\n",
    "    final_prod = final_prod.drop_duplicates('lems', keep='first')\n",
    "    final_prod = final_prod.drop_duplicates('tokens', keep='first')\n",
    "\n",
    "    final_prod['title'] = final_prod['title'].astype(str)\n",
    "\n",
    "    final_prod['competitor_evt_indchar'] = ['Yes' if(len(np.intersect1d(x,competitor_keywords)) > 0) else 'No' for x in final_prod['tokens']]\n",
    "\n",
    "    counter_guid = int(date.today().strftime(\"%Y%m%d\"))\n",
    "    final_prod['efsevt_guid'] = [(counter_guid*1000)+i for i in range(len(final_prod))]\n",
    "\n",
    "    print(final_prod.dtypes)\n",
    "    print(final_prod_events.dtypes)\n",
    "\n",
    "    foriegn_key = []\n",
    "    for index, row in final_prod.iterrows():\n",
    "        flag = False\n",
    "        for index_event, row_event in final_prod_events.iterrows():\n",
    "            if(row['keyword'] != '' ):\n",
    "                if(row['keyword'] in row_event['NAME']):\n",
    "                    print(row['keyword'],row_event['NAME'])\n",
    "                    foriegn_key.append(row_event['GUID'])\n",
    "                    flag = True\n",
    "        if(flag == False):\n",
    "            foriegn_key.append(0)\n",
    "\n",
    "    print(foriegn_key)\n",
    "\n",
    "    final_prod['guid'] = [(counter_guid*2000)+i for i in range(len(final_prod))]\n",
    "    final_prod['fixed_annual_ind'] = 'n'\n",
    "    final_prod['perm_env_ind'] = 'n'\n",
    "    final_prod['cancelled_ind'] = 'n'\n",
    "    final_prod['create_user'] = ''\n",
    "    final_prod['update_user'] = ''\n",
    "    final_prod['perm_env_ind'] = 'n'\n",
    "    final_prod['crt_timestamp'] = date.today()\n",
    "    final_prod['upd_timestamp'] = date.today()\n",
    "\n",
    "    final_prod.rename(columns = {'link':'source_of_event'}, inplace = True)\n",
    "    final_prod[[\"datetime\"]] = final_prod[[\"datetime\"]].astype(str)\n",
    "    final_prod.columns = final_prod.columns.str.upper()\n",
    "\n",
    "    final_prod.to_csv('Events.csv', mode='a', index=False, header=False)\n",
    "    return final_prod\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from geopy.geocoders import Photon, GoogleV3, Nominatim\n",
    "import pgeocode\n",
    "from math import cos, asin, sqrt, pi\n",
    "\n",
    "def distance(lat1, lon1, lat2, lon2):\n",
    "    p = pi/180\n",
    "    a = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p) * cos(lat2*p) * (1-cos((lon2-lon1)*p))/2\n",
    "    return round(12742 * asin(sqrt(a)),2)\n",
    "\n",
    "def place_distance(string1,string2):\n",
    "    try:\n",
    "        geolocator_addr = Nominatim(user_agent=\"efs\")\n",
    "        place = string1 + \",\" + string2\n",
    "        place_2 = \"Waitrose,\" + string2\n",
    "        pin = geolocator_addr.geocode(place)\n",
    "        pin_2 = geolocator_addr.geocode(place_2)\n",
    "        print(pin)\n",
    "        print(pin_2)\n",
    "        print(pin.raw['lat'],pin.raw['lon'],pin_2.raw['lat'],pin_2.raw['lon'])\n",
    "    except:\n",
    "        return 'N/A'\n",
    "\n",
    "    return distance(float(pin.raw['lat']),float(pin.raw['lon']),float(pin_2.raw['lat']),float(pin_2.raw['lon']))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "road = []\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "\n",
    "def distance_infrastructure(lat1, lon1, lat2, lon2):\n",
    "    p = pi/180\n",
    "    a = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p) * cos(lat2*p) * (1-cos((lon2-lon1)*p))/2\n",
    "    return round(12742 * asin(sqrt(a)),2)\n",
    "\n",
    "def place_distance_infrastructure(string2, work):\n",
    "    temp = []\n",
    "    try:\n",
    "        route_df = pd.read_csv(\"Branch_Lat_Lon.csv\")\n",
    "        geolocator_addr = Nominatim(user_agent=\"http\")\n",
    "        place_2 = string2 + \", London, UK\"\n",
    "        pin_2 = geolocator_addr.geocode(place_2)\n",
    "        print(pin_2)\n",
    "        print(pin_2.raw['lat'],pin_2.raw['lon'])\n",
    "        actual = 999999\n",
    "        if(pin_2 != 'None'):\n",
    "            for index, row in route_df.iterrows():\n",
    "                dif = distance_infrastructure(float(pin_2.raw['lat']),float(pin_2.raw['lon']),row[\"lat\"],row[\"lon\"])\n",
    "                if( dif < 20):\n",
    "                    if(actual > dif):\n",
    "                        actual = dif\n",
    "                        branch_name = row[\"branch\"]\n",
    "            if(actual <= 1):\n",
    "                print(actual)\n",
    "                temp.append(branch_name)\n",
    "                print(branch_name)\n",
    "                print(\"Yes\")\n",
    "                road.append([ string2 + \" \" + work, branch_name, \"\", date.today(), branch_keyword_bu_num[branch_name], actual,\"\"])\n",
    "                return branch_name\n",
    "        print(actual)\n",
    "    except:\n",
    "        n = 0\n",
    "        return None\n",
    "\n",
    "def infrastructure():\n",
    "    url = \"https://tfl.gov.uk/traffic/status/?Input=&lineIds=&dateTypeSelect=Future%20date&direction=&startDate=\"+date.today().strftime(\"%Y-%m-%d\")+\"T00%3A00%3A00&endDate=\"+date.today().strftime(\"%Y-%m-%d\")+\"T23%3A59%3A59&lat=51.50721740722656&lng=-0.12758620083332062&placeType=stoppoint&input=London%2C%20UK\"\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "    ele = soup.select('div[class^=\"road-disruption\"]')\n",
    "    street = []\n",
    "    works = []\n",
    "    for element in ele:\n",
    "        h2_tags = element.select('h4')\n",
    "        p_tags = element.select('p[class^=\"topmargin\"]')\n",
    "        date_tags = element.select('p[class^=\"highlight dates\"]')\n",
    "        print(date_tags[0].text.strip(\"\\n\\n\").split(\"\\n\"))\n",
    "        for h2_tag,p_tag in zip(h2_tags,p_tags):\n",
    "            if(\"Works\" in p_tag.text):\n",
    "                arr = h2_tag.text.strip().split(\" \")\n",
    "                word = \"\"\n",
    "                for i in range(1,len(arr)-1):\n",
    "                    if('(' not in arr[i]):\n",
    "                        word = word + \" \" + arr[i]\n",
    "                street.append(word)\n",
    "                works.append(p_tag.text)\n",
    "    street = list(set(street))\n",
    "    print(len(street))\n",
    "    for i,j in zip(street,works):\n",
    "        place_distance_infrastructure(i,j)\n",
    "    return road\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import smtplib, ssl\n",
    "from smtplib import SMTP\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from pretty_html_table import build_table\n",
    "from pyshorteners import Shortener\n",
    "from io import BytesIO\n",
    "from email.mime.application import MIMEApplication\n",
    "from cryptography.fernet import Fernet\n",
    "\n",
    "def mail_data(final_prod, final_prod_old):\n",
    "    port = 465 # For SSL\n",
    "    context = ssl.create_default_context()\n",
    "    mail_df = pd.DataFrame()\n",
    "\n",
    "    mail_df[\"TITLE\"] = final_prod[\"TITLE\"]\n",
    "    mail_df[\"BRANCH\"] = final_prod[\"BRANCH\"]\n",
    "    mail_df[\"SOURCE\"] = final_prod[\"MEDIA\"]\n",
    "    mail_df[\"DATETIME\"] = final_prod[\"DATETIME\"]\n",
    "    mail_df[\"BRANCH_NUM\"] = final_prod[\"BU_NUM\"]\n",
    "\n",
    "    distance_arr = []\n",
    "    for index, row in final_prod.iterrows():\n",
    "        if(row[\"KEYWORD\"] and row[\"BRANCH\"]):\n",
    "            distance_arr.append(place_distance(row[\"KEYWORD\"], row[\"BRANCH\"]))\n",
    "    mail_df[\"DISTANCE IN MILES\"] = distance_arr\n",
    "\n",
    "    urls = []\n",
    "    for index,row in final_prod.iterrows():\n",
    "        x = Shortener().tinyurl.short(row[\"SOURCE_OF_EVENT\"])\n",
    "        urls.append(x)\n",
    "    mail_df[\"LINK\"] = urls\n",
    "\n",
    "    for index, row in mail_df.iterrows():\n",
    "        if(row[\"DISTANCE IN MILES\"] != 'N/A'):\n",
    "            if(row[\"DISTANCE IN MILES\"] > 25):\n",
    "                mail_df.drop(index=index, axis=0, inplace=True)\n",
    "\n",
    "    all_prod = pd.concat([mail_df, final_prod_old])\n",
    "    road_list = infrastructure()\n",
    "    print(road_list)\n",
    "    road_df = pd.DataFrame(road_list, columns = [\"TITLE\", \"BRANCH\", \"SOURCE\", \"DATETIME\", \"BRANCH_NUM\", \"DISTANCE IN MILES\", \"LINK\"])\n",
    "    road_df = road_df.drop_duplicates(\"BRANCH\", keep=\"first\")\n",
    "\n",
    "    html_table = mail_df.to_html(index=False, classes='example-table')\n",
    "    road_table = road_df.to_html(index=False, classes='example-table')\n",
    "\n",
    "    text = f'''Hello Alex and Tim,\\n Herewith attaching the events captured for all the competitors (core event types) including all the branches from {(date.today() - timedelta(days = 1)).strftime('%d-%m-%Y')} to {date.today().strftime('%d-%m-%Y')} which are auto-generated from the script.\\n\\n\\nThanks And Regards,\\nSubhash\\n\\n\\n\\n'''\n",
    "\n",
    "    html_table = html_table.replace('<th>', '<th style=\"padding: 10px 90px 10px 90px;\">', 1)\n",
    "    road_table = road_table.replace('<th>', '<th style=\"padding: 10px 80px 10px 80px;\">', 1)\n",
    "\n",
    "    if(mail_df.empty):\n",
    "        html_table\n",
    "\n",
    "    html = f'''\n",
    "<html>\n",
    "<head>\n",
    " <style>\n",
    " table.example-table th{{\n",
    "  padding: 10px;\n",
    "  text-align: center;\n",
    "  background-color: #FFFFFF;\n",
    "  font-weight: bold;\n",
    "  font-size: 14px;\n",
    "  width: 400px;\n",
    " }}\n",
    " table.example-table th:first-child {{\n",
    "  padding: 20px 100px 20px 100px;\n",
    " }}\n",
    " table.example-table td {{\n",
    "  padding: 5px;\n",
    "  color: black;\n",
    "  font-size: 12px;\n",
    "  width: 400px;\n",
    "  font-family: Century Gothic, sans-serif;\n",
    " }}\n",
    " </style>\n",
    "</head>\n",
    "<body>\n",
    " <pre>{text}</pre>\n",
    " {html_table}\n",
    " <br/>\n",
    " <br/>\n",
    " {road_table}\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "    part1 = MIMEText(html, 'html')\n",
    "    msg = MIMEMultipart(\"alternative\")\n",
    "    msg['Subject'] = \"Automated Event Capturing Model\"\n",
    "    recipients = ['subhash.verma@johnlewis.co.uk']\n",
    "    msg['To'] = \", \".join(recipients)\n",
    "    msg.attach(part1)\n",
    "\n",
    "    file_name = date.today().strftime(\"%d-%m-%Y\") + \"_Events.xlsx\"\n",
    "    textStream = BytesIO()\n",
    "    writer = pd.ExcelWriter(textStream, engine='xlsxwriter')\n",
    "    all_prod.to_excel(writer,sheet_name=\"Competitor Events\",index=False)\n",
    "    road_df.to_excel(writer,sheet_name=\"Road Closure Events\",index=False)\n",
    "    writer.close()\n",
    "    textStream.seek(0)\n",
    "    attachment = MIMEApplication(textStream.read(), name= file_name)\n",
    "    attachment['Content-Disposition'] = 'attachment; filename=\"{}\"'.format(file_name)\n",
    "    msg.attach(attachment)\n",
    "\n",
    "    with smtplib.SMTP_SSL(\"smtp.gmail.com\", port, context=context) as server:\n",
    "        dec = str(Fernet('egupkHT3QJHG1c5dcPGiWEZaWdH04_uhgyD-8lYNxWM=').decrypt(b'gAAAAABpHy2IRPVaNZJU3a2jDD68rGtj0jMYEvJyrWRJepy-wUXuHwKdmAzMTSDXAWkP4S8tUWCd6Q5egqHWKGFkMx18sIu6NUPerPx9TSkeFpCedLP3LAc='), 'UTF-8')\n",
    "        print(\"IN\")\n",
    "        server.login(\"subhash.verma@johnlewis.co.uk\", dec)\n",
    "        server.sendmail(\"subhash.verma@johnlewis.co.uk\", recipients, msg.as_string())\n",
    "\n",
    "    return all_prod, road_df\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import gspread\n",
    "if __name__ == '__main__':\n",
    "    final_prod = outsource_news()\n",
    "    final_prod_old = pd.DataFrame()\n",
    "    all_prod, road_df = mail_data(final_prod, final_prod_old)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
